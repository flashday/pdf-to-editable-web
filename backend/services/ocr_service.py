"""
PaddleOCR PP-Structure integration service with error handling and preprocessing

PaddleOCR ç‰ˆæœ¬å…¼å®¹æ€§ï¼š
- æ”¯æŒ PaddleOCR 3.x (æ¨è 3.3.3)
- æ”¯æŒ PaddlePaddle 3.x (æ¨è 3.2.2ï¼Œæ³¨æ„ 3.3.0 æœ‰ oneDNN å…¼å®¹æ€§é—®é¢˜)
- å‘åå…¼å®¹ PaddleOCR 2.x

ä¸»è¦ API å˜åŒ–ï¼ˆ3.x vs 2.xï¼‰ï¼š
- PaddleOCR ç±»ï¼šåŸºæœ¬å…¼å®¹ï¼Œuse_structure å‚æ•°å·²åºŸå¼ƒ
- PPStructure ç±»ï¼šlayout_score_threshold/layout_nms_threshold å‚æ•°å·²ç§»é™¤
- ç»“æœæ ¼å¼ï¼šåŸºæœ¬å…¼å®¹

æ¨¡å‹ç¼“å­˜ç­–ç•¥ï¼š
- PPStructureV3 å®ä¾‹åœ¨æ¨¡å—çº§åˆ«ç¼“å­˜ï¼Œé¿å…é‡å¤åŠ è½½
- æ”¯æŒå¯åŠ¨æ—¶é¢„åŠ è½½æ¨¡å‹

CPU æ€§èƒ½ä¼˜åŒ–ï¼š
- çº¿ç¨‹æ•°é…ç½®ï¼šæ ¹æ® Intel CPU ç‰¹æ€§ä¼˜åŒ–
- å‚è€ƒï¼šMDFiles/implementation/PADDLEOCR_CPU_PERFORMANCE_OPTIMIZATION.md
"""
import os
import logging
import threading

# ============================================================================
# CPU æ€§èƒ½ä¼˜åŒ– - çº¿ç¨‹é…ç½®ï¼ˆå¿…é¡»åœ¨å¯¼å…¥ paddle ä¹‹å‰è®¾ç½®ï¼‰
# é’ˆå¯¹ Intel Core Ultra 7 / i7 ä¼˜åŒ–
# æ³¨æ„ï¼šæŸäº› Intel CPU ä¸Šå•çº¿ç¨‹å¯èƒ½æ›´å¿«ï¼Œå»ºè®®è¿›è¡ŒåŸºå‡†æµ‹è¯•
# ============================================================================
# CPU çº¿ç¨‹è®¾ç½®ï¼šé»˜è®¤ 8 çº¿ç¨‹ï¼ˆé€‚åˆ Intel i7/Ultra 7ï¼‰
# å¦‚éœ€è°ƒæ•´ï¼Œå¯è®¾ç½®ç¯å¢ƒå˜é‡ PADDLEOCR_CPU_THREADS
_CPU_THREADS = os.environ.get('PADDLEOCR_CPU_THREADS', '8')
os.environ.setdefault('OMP_NUM_THREADS', _CPU_THREADS)
os.environ.setdefault('MKL_NUM_THREADS', _CPU_THREADS)
# OpenBLAS çº¿ç¨‹è®¾ç½®
os.environ.setdefault('OPENBLAS_NUM_THREADS', _CPU_THREADS)
# ç¦ç”¨ oneDNN è¯¦ç»†æ—¥å¿—
os.environ.setdefault('DNNL_VERBOSE', '0')
os.environ.setdefault('MKLDNN_VERBOSE', '0')
from typing import List, Optional, Dict, Any, Tuple
from pathlib import Path
import numpy as np
from PIL import Image, ImageEnhance, ImageFilter
import cv2

from backend.models.document import LayoutResult, Region, TableStructure, BoundingBox, RegionType
from backend.services.interfaces import OCRServiceInterface
from backend.services.retry_handler import retry_handler, RetryConfig, NetworkRetryError

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ============================================================================
# æ¨¡å‹ç¼“å­˜ - å•ä¾‹æ¨¡å¼ï¼Œé¿å…é‡å¤åŠ è½½æ¨¡å‹
# ============================================================================
_ppstructure_v3_instance = None
_ppstructure_v3_lock = threading.Lock()
_models_loaded = False
_models_loading = False


def get_ppstructure_v3_instance():
    """
    è·å– PPStructureV3 çš„å•ä¾‹å®ä¾‹
    
    ä½¿ç”¨åŒé‡æ£€æŸ¥é”å®šæ¨¡å¼ç¡®ä¿çº¿ç¨‹å®‰å…¨
    æ¨¡å‹åªä¼šåœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶åŠ è½½ï¼Œåç»­è°ƒç”¨ç›´æ¥è¿”å›ç¼“å­˜çš„å®ä¾‹
    
    Returns:
        PPStructureV3 å®ä¾‹ï¼Œå¦‚æœä¸å¯ç”¨åˆ™è¿”å› None
    """
    global _ppstructure_v3_instance
    
    if _ppstructure_v3_instance is not None:
        return _ppstructure_v3_instance
    
    with _ppstructure_v3_lock:
        # åŒé‡æ£€æŸ¥
        if _ppstructure_v3_instance is not None:
            return _ppstructure_v3_instance
        
        try:
            from paddleocr import PPStructureV3
            logger.info("æ­£åœ¨åŠ è½½ PPStructureV3 æ¨¡å‹ï¼ˆé¦–æ¬¡åŠ è½½ï¼Œè¯·è€å¿ƒç­‰å¾…ï¼‰...")
            import time
            start_time = time.time()
            _ppstructure_v3_instance = PPStructureV3()
            elapsed = time.time() - start_time
            logger.info(f"PPStructureV3 æ¨¡å‹åŠ è½½å®Œæˆï¼Œè€—æ—¶ {elapsed:.1f} ç§’")
            return _ppstructure_v3_instance
        except ImportError as e:
            logger.warning(f"PPStructureV3 ä¸å¯ç”¨: {e}")
            return None
        except Exception as e:
            logger.error(f"PPStructureV3 åŠ è½½å¤±è´¥: {e}")
            return None


def preload_models():
    """
    é¢„åŠ è½½æ‰€æœ‰ OCR æ¨¡å‹
    
    åœ¨åç«¯å¯åŠ¨æ—¶è°ƒç”¨æ­¤å‡½æ•°ï¼Œç¡®ä¿æ¨¡å‹åœ¨æ¥æ”¶è¯·æ±‚å‰å·²åŠ è½½å®Œæˆ
    è¿™æ ·ç”¨æˆ·ä¸Šä¼  PDF æ—¶ä¸éœ€è¦ç­‰å¾…æ¨¡å‹åŠ è½½
    
    é‡è¦ï¼šPPStructureV3 å†…éƒ¨çš„æ¨¡å‹æ˜¯æ‡’åŠ è½½çš„ï¼Œä»…åˆ›å»ºå®ä¾‹ä¸ä¼šåŠ è½½æ¨¡å‹
    å¿…é¡»è°ƒç”¨ predict() æ–¹æ³•æ‰èƒ½è§¦å‘å†…éƒ¨æ¨¡å‹çš„åŠ è½½
    
    æ³¨æ„ï¼šPPStructureV3 å·²ç»åŒ…å«äº†å®Œæ•´çš„ OCR åŠŸèƒ½ï¼Œä¸éœ€è¦å•ç‹¬åŠ è½½ PaddleOCR
    
    Returns:
        bool: æ¨¡å‹æ˜¯å¦åŠ è½½æˆåŠŸ
    """
    global _models_loaded, _models_loading
    
    if _models_loaded:
        logger.info("æ¨¡å‹å·²åŠ è½½ï¼Œè·³è¿‡é¢„åŠ è½½")
        return True
    
    if _models_loading:
        logger.info("æ¨¡å‹æ­£åœ¨åŠ è½½ä¸­...")
        return False
    
    _models_loading = True
    logger.info("=" * 60)
    logger.info("å¼€å§‹é¢„åŠ è½½ OCR æ¨¡å‹...")
    logger.info("=" * 60)
    
    import time
    total_start = time.time()
    
    try:
        # é¢„åŠ è½½ PPStructureV3ï¼ˆåŒ…å«å¸ƒå±€åˆ†æã€è¡¨æ ¼è¯†åˆ«ã€OCR ç­‰å¤šä¸ªæ¨¡å‹ï¼‰
        # PPStructureV3 å·²ç»åŒ…å«äº†å®Œæ•´çš„ OCR åŠŸèƒ½ï¼Œä¸éœ€è¦å•ç‹¬åŠ è½½ PaddleOCR
        logger.info("åŠ è½½ PPStructureV3 æ¨¡å‹ï¼ˆåŒ…å« OCR åŠŸèƒ½ï¼‰...")
        ppstructure = get_ppstructure_v3_instance()
        if ppstructure is None:
            logger.warning("PPStructureV3 åŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨å›é€€æ–¹æ¡ˆ")
        else:
            # é‡è¦ï¼šPPStructureV3 å†…éƒ¨æ¨¡å‹æ˜¯æ‡’åŠ è½½çš„
            # å¿…é¡»è°ƒç”¨ predict() æ‰èƒ½è§¦å‘å†…éƒ¨æ¨¡å‹ï¼ˆPP-LCNet, PP-OCRv5 ç­‰ï¼‰çš„åŠ è½½
            # åˆ›å»ºä¸€ä¸ªå°çš„æµ‹è¯•å›¾åƒæ¥è§¦å‘æ¨¡å‹åŠ è½½
            logger.info("è§¦å‘ PPStructureV3 å†…éƒ¨æ¨¡å‹åŠ è½½ï¼ˆé¦–æ¬¡ predict è°ƒç”¨ï¼‰...")
            try:
                # åˆ›å»ºä¸€ä¸ª 100x100 çš„ç™½è‰²æµ‹è¯•å›¾åƒ
                test_image = np.ones((100, 100, 3), dtype=np.uint8) * 255
                # æ·»åŠ ä¸€äº›æ–‡å­—åŒºåŸŸï¼ˆé»‘è‰²çŸ©å½¢ï¼‰ä»¥è§¦å‘ OCR æ¨¡å‹
                test_image[20:40, 20:80] = 0
                
                # ä¿å­˜ä¸´æ—¶æµ‹è¯•å›¾åƒ
                import tempfile
                with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp:
                    tmp_path = tmp.name
                    Image.fromarray(test_image).save(tmp_path)
                
                # è°ƒç”¨ predict è§¦å‘å†…éƒ¨æ¨¡å‹åŠ è½½
                warmup_start = time.time()
                # ä½¿ç”¨ä¸å®é™…å¤„ç†ç›¸åŒçš„å‚æ•°ï¼Œç¦ç”¨ä¸å¿…è¦çš„åŠŸèƒ½
                _ = list(ppstructure.predict(
                    tmp_path,
                    use_doc_orientation_classify=False,
                    use_doc_unwarping=False,
                    use_seal_recognition=False,
                    use_formula_recognition=False,
                    use_chart_recognition=False
                ))
                warmup_elapsed = time.time() - warmup_start
                logger.info(f"PPStructureV3 å†…éƒ¨æ¨¡å‹åŠ è½½å®Œæˆï¼Œè€—æ—¶ {warmup_elapsed:.1f} ç§’")
                
                # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                try:
                    os.remove(tmp_path)
                except:
                    pass
                    
            except Exception as e:
                logger.warning(f"PPStructureV3 é¢„çƒ­å¤±è´¥: {e}")
                import traceback
                logger.warning(traceback.format_exc())
        
        total_elapsed = time.time() - total_start
        logger.info("=" * 60)
        logger.info(f"æ‰€æœ‰æ¨¡å‹é¢„åŠ è½½å®Œæˆï¼æ€»è€—æ—¶: {total_elapsed:.1f} ç§’")
        logger.info("=" * 60)
        
        _models_loaded = True
        _models_loading = False
        return True
        
    except Exception as e:
        logger.error(f"æ¨¡å‹é¢„åŠ è½½å¤±è´¥: {e}")
        _models_loading = False
        return False


def is_models_loaded() -> bool:
    """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½å®Œæˆ"""
    return _models_loaded


def is_models_loading() -> bool:
    """æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ­£åœ¨åŠ è½½ä¸­"""
    return _models_loading


# ============================================================================
# PaddleOCR åŸºç¡€å¼•æ“ç¼“å­˜
# ============================================================================
_paddleocr_instance = None
_paddleocr_lock = threading.Lock()


def get_paddleocr_instance(lang: str = 'ch'):
    """
    è·å– PaddleOCR åŸºç¡€å¼•æ“çš„å•ä¾‹å®ä¾‹
    
    Args:
        lang: è¯­è¨€è®¾ç½®
        
    Returns:
        PaddleOCR å®ä¾‹
    """
    global _paddleocr_instance
    
    if _paddleocr_instance is not None:
        return _paddleocr_instance
    
    with _paddleocr_lock:
        if _paddleocr_instance is not None:
            return _paddleocr_instance
        
        try:
            from paddleocr import PaddleOCR
            import paddleocr
            
            version = getattr(paddleocr, '__version__', '2.0.0')
            is_v3 = version.startswith('3.')
            
            logger.info(f"æ­£åœ¨åŠ è½½ PaddleOCR åŸºç¡€å¼•æ“ (ç‰ˆæœ¬: {version})...")
            import time
            start_time = time.time()
            
            if is_v3:
                # å…³é—­æ–¹å‘åˆ†ç±»å™¨ï¼ˆä¸éœ€è¦å¤„ç†æ—‹è½¬æ–‡æ¡£ï¼‰ï¼Œæ˜¾å¼è®¾ç½® det_limit_side_len=960
                _paddleocr_instance = PaddleOCR(
                    use_textline_orientation=False,  # å…³é—­æ–¹å‘åˆ†ç±»ï¼Œæé€Ÿ 10-20%
                    lang=lang,
                    det_limit_side_len=960  # æ˜¾å¼è®¾ç½®æ£€æµ‹å›¾åƒæœ€å¤§è¾¹é•¿
                )
            else:
                # å…³é—­æ–¹å‘åˆ†ç±»å™¨ï¼Œæ˜¾å¼è®¾ç½® det_limit_side_len=960
                _paddleocr_instance = PaddleOCR(
                    use_angle_cls=False,  # å…³é—­æ–¹å‘åˆ†ç±»ï¼Œæé€Ÿ 10-20%
                    lang=lang,
                    use_gpu=False,
                    show_log=False,
                    det_limit_side_len=960  # æ˜¾å¼è®¾ç½®æ£€æµ‹å›¾åƒæœ€å¤§è¾¹é•¿
                )
            
            elapsed = time.time() - start_time
            logger.info(f"PaddleOCR åŸºç¡€å¼•æ“åŠ è½½å®Œæˆï¼Œè€—æ—¶ {elapsed:.1f} ç§’")
            return _paddleocr_instance
            
        except Exception as e:
            logger.error(f"PaddleOCR åŸºç¡€å¼•æ“åŠ è½½å¤±è´¥: {e}")
            return None

class OCRProcessingError(Exception):
    """Custom exception for OCR processing errors"""
    pass

class PaddleOCRService(OCRServiceInterface):
    """
    PaddleOCR PP-Structure service wrapper with error handling and preprocessing
    """
    
    def __init__(self, use_gpu: bool = False, lang: str = 'ch'):
        """
        Initialize PaddleOCR service
        
        Args:
            use_gpu: Whether to use GPU acceleration (default: False for CPU-only)
            lang: Language for OCR recognition (default: 'ch' for Chinese/English)
        """
        self.use_gpu = use_gpu
        self.lang = lang
        self._ocr_engine = None
        self._structure_engine = None
        
        # ç¼“å­˜ PPStructure çš„ç»“æœï¼Œé¿å…é‡å¤å¤„ç†
        # key: image_path, value: processed_result
        self._ppstructure_result_cache = {}
        
        # Initialize engines lazily to avoid import errors during testing
        self._initialize_engines()
    
    def _initialize_engines(self):
        """Initialize PaddleOCR engines with error handling
        
        ä½¿ç”¨ç¼“å­˜çš„å•ä¾‹å®ä¾‹ï¼Œé¿å…é‡å¤åŠ è½½æ¨¡å‹
        PaddleOCR 3.x ä¸­ï¼ŒPPStructureV3 å·²ç»åŒ…å«äº†å®Œæ•´çš„ OCR åŠŸèƒ½ï¼Œ
        ä¸éœ€è¦å•ç‹¬åˆ›å»º PaddleOCR å®ä¾‹
        """
        try:
            import paddleocr
            version = getattr(paddleocr, '__version__', '2.0.0')
            is_v3 = version.startswith('3.')
            
            if is_v3:
                # PaddleOCR 3.x: ä½¿ç”¨ç¼“å­˜çš„ PPStructureV3 å®ä¾‹
                # PPStructureV3 å·²ç»åŒ…å«äº† OCR åŠŸèƒ½ï¼Œä¸éœ€è¦å•ç‹¬çš„ PaddleOCR å®ä¾‹
                ppstructure = get_ppstructure_v3_instance()
                if ppstructure is not None:
                    # ä½¿ç”¨ PPStructureV3 ä½œä¸ºä¸»å¼•æ“
                    self._ocr_engine = ppstructure
                    self._structure_engine = ppstructure
                    logger.info("ä½¿ç”¨ç¼“å­˜çš„ PPStructureV3 ä½œä¸º OCR å¼•æ“")
                    return
                
                # å¦‚æœ PPStructureV3 ä¸å¯ç”¨ï¼Œå›é€€åˆ° PaddleOCR
                cached_ocr = get_paddleocr_instance(self.lang)
                if cached_ocr is not None:
                    self._ocr_engine = cached_ocr
                    self._structure_engine = cached_ocr
                    logger.info("ä½¿ç”¨ç¼“å­˜çš„ PaddleOCR å¼•æ“å®ä¾‹")
                    return
            
            # PaddleOCR 2.x æˆ–ç¼“å­˜ä¸å¯ç”¨æ—¶ï¼Œåˆ›å»ºæ–°å®ä¾‹
            logger.warning("ç¼“å­˜å®ä¾‹ä¸å¯ç”¨ï¼Œåˆ›å»ºæ–°çš„ PaddleOCR å®ä¾‹...")
            from paddleocr import PaddleOCR
            
            if is_v3:
                # å…³é—­æ–¹å‘åˆ†ç±»å™¨ï¼ˆä¸éœ€è¦å¤„ç†æ—‹è½¬æ–‡æ¡£ï¼‰ï¼Œæ˜¾å¼è®¾ç½® det_limit_side_len=960
                self._ocr_engine = PaddleOCR(
                    use_textline_orientation=False,  # å…³é—­æ–¹å‘åˆ†ç±»ï¼Œæé€Ÿ 10-20%
                    lang=self.lang,
                    det_limit_side_len=960  # æ˜¾å¼è®¾ç½®æ£€æµ‹å›¾åƒæœ€å¤§è¾¹é•¿
                )
            else:
                # å…³é—­æ–¹å‘åˆ†ç±»å™¨ï¼Œæ˜¾å¼è®¾ç½® det_limit_side_len=960
                self._ocr_engine = PaddleOCR(
                    use_angle_cls=False,  # å…³é—­æ–¹å‘åˆ†ç±»ï¼Œæé€Ÿ 10-20%
                    lang=self.lang,
                    use_gpu=self.use_gpu,
                    show_log=False,
                    det_limit_side_len=960  # æ˜¾å¼è®¾ç½®æ£€æµ‹å›¾åƒæœ€å¤§è¾¹é•¿
                )
            
            self._structure_engine = self._ocr_engine
            logger.info(f"PaddleOCR engines initialized (version: {version})")
            
        except Exception as e:
            raise OCRProcessingError(f"Engine initialization failed: {e}")
    
    def _convert_v3_result_to_legacy(self, v3_results: List) -> List:
        """
        å°† PaddleOCR 3.x çš„ OCRResult æ ¼å¼è½¬æ¢ä¸º 2.x çš„æ—§æ ¼å¼
        
        PaddleOCR 3.x è¿”å›æ ¼å¼:
        - OCRResult å¯¹è±¡ï¼ŒåŒ…å« dt_polys, rec_texts, rec_scores ç­‰å±æ€§
        - dt_polys: numpy array, shape (N, 4, 2) - Nä¸ªæ£€æµ‹æ¡†ï¼Œæ¯ä¸ªæ¡†4ä¸ªç‚¹ï¼Œæ¯ä¸ªç‚¹2ä¸ªåæ ‡
        
        PaddleOCR 2.x è¿”å›æ ¼å¼:
        - [[[bbox_points, (text, score)], ...]]
        - bbox_points: [[x1,y1], [x2,y2], [x3,y3], [x4,y4]]
        
        Args:
            v3_results: PaddleOCR 3.x predict æ–¹æ³•è¿”å›çš„ç»“æœåˆ—è¡¨
            
        Returns:
            è½¬æ¢ä¸º 2.x æ ¼å¼çš„ç»“æœ
        """
        legacy_results = []
        
        for result in v3_results:
            page_results = []
            
            # è·å–æ£€æµ‹æ¡†ã€æ–‡æœ¬å’Œç½®ä¿¡åº¦
            if isinstance(result, dict):
                dt_polys = result.get('dt_polys', [])
                rec_texts = result.get('rec_texts', [])
                rec_scores = result.get('rec_scores', [])
            else:
                dt_polys = getattr(result, 'dt_polys', [])
                rec_texts = getattr(result, 'rec_texts', [])
                rec_scores = getattr(result, 'rec_scores', [])
            
            # è½¬æ¢ä¸ºæ—§æ ¼å¼
            for i, poly in enumerate(dt_polys):
                text = rec_texts[i] if i < len(rec_texts) else ''
                score = rec_scores[i] if i < len(rec_scores) else 0.0
                
                # å°† numpy æ•°ç»„è½¬æ¢ä¸ºåˆ—è¡¨
                # poly çš„å½¢çŠ¶æ˜¯ (4, 2)ï¼Œå³ 4 ä¸ªç‚¹ï¼Œæ¯ä¸ªç‚¹ 2 ä¸ªåæ ‡
                if hasattr(poly, 'tolist'):
                    poly_list = poly.tolist()
                else:
                    poly_list = list(poly)
                
                # ç¡®ä¿æ˜¯æ­£ç¡®çš„æ ¼å¼: [[x1,y1], [x2,y2], [x3,y3], [x4,y4]]
                if len(poly_list) == 4 and len(poly_list[0]) == 2:
                    bbox_points = poly_list
                else:
                    # å°è¯•å…¶ä»–æ ¼å¼è½¬æ¢
                    logger.warning(f"Unexpected poly format: {poly_list}")
                    continue
                
                # æ—§æ ¼å¼: [bbox_points, (text, confidence)]
                page_results.append([bbox_points, (text, float(score))])
            
            legacy_results.append(page_results)
        
        return legacy_results
    
    def preprocess_image(self, image_path: str, output_path: Optional[str] = None) -> Tuple[str, Dict[str, Any]]:
        """
        Preprocess image for optimal OCR results
        
        Args:
            image_path: Path to input image
            output_path: Path for preprocessed image (optional)
            
        Returns:
            Tuple of (path to preprocessed image, scale info dict)
        """
        try:
            # Load image
            image = Image.open(image_path)
            original_width, original_height = image.size
            
            # Convert to RGB if necessary
            if image.mode != 'RGB':
                image = image.convert('RGB')
            
            # Apply preprocessing steps
            image = self._enhance_image_quality(image)
            image, scale_info = self._normalize_image_size_with_scale(image)
            
            # Record original dimensions for coordinate mapping
            scale_info['original_width'] = original_width
            scale_info['original_height'] = original_height
            
            # Save preprocessed image
            if output_path is None:
                base_path = Path(image_path)
                output_path = str(base_path.parent / f"{base_path.stem}_preprocessed{base_path.suffix}")
            
            image.save(output_path, quality=95, optimize=True)
            
            logger.info(f"Image preprocessed and saved to: {output_path}, scale_info: {scale_info}")
            return output_path, scale_info
            
        except Exception as e:
            raise OCRProcessingError(f"Image preprocessing failed: {e}")
    
    def _enhance_image_quality(self, image: Image.Image) -> Image.Image:
        """
        Enhance image quality for better OCR results
        
        Args:
            image: PIL Image object
            
        Returns:
            Enhanced PIL Image object
        """
        # Enhance contrast
        enhancer = ImageEnhance.Contrast(image)
        image = enhancer.enhance(1.2)
        
        # Enhance sharpness
        enhancer = ImageEnhance.Sharpness(image)
        image = enhancer.enhance(1.1)
        
        # Apply slight denoising
        image = image.filter(ImageFilter.MedianFilter(size=3))
        
        return image
    
    def _normalize_image_size_with_scale(self, image: Image.Image, max_dimension: int = 1280) -> Tuple[Image.Image, Dict[str, Any]]:
        """
        Normalize image size to optimal dimensions for OCR and return scale info
        
        Args:
            image: PIL Image object
            max_dimension: Maximum dimension for resizing
            
        Returns:
            Tuple of (Resized PIL Image object, scale info dict)
        """
        width, height = image.size
        scale_info = {
            'preprocessed_width': width,
            'preprocessed_height': height,
            'scale_x': 1.0,
            'scale_y': 1.0,
            'was_resized': False
        }
        
        # Only resize if image is too large
        if max(width, height) > max_dimension:
            if width > height:
                new_width = max_dimension
                new_height = int(height * (max_dimension / width))
            else:
                new_height = max_dimension
                new_width = int(width * (max_dimension / height))
            
            # Calculate scale factors (from preprocessed back to original)
            scale_info['scale_x'] = width / new_width
            scale_info['scale_y'] = height / new_height
            scale_info['preprocessed_width'] = new_width
            scale_info['preprocessed_height'] = new_height
            scale_info['was_resized'] = True
            
            image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)
            logger.info(f"Image resized from {width}x{height} to {new_width}x{new_height}, scale: {scale_info['scale_x']:.3f}x{scale_info['scale_y']:.3f}")
        
        return image, scale_info
    
    def _normalize_image_size(self, image: Image.Image, max_dimension: int = 1280) -> Image.Image:
        """
        Normalize image size to optimal dimensions for OCR (legacy method)
        
        Args:
            image: PIL Image object
            max_dimension: Maximum dimension for resizing
            
        Returns:
            Resized PIL Image object
        """
        width, height = image.size
        
        # Only resize if image is too large
        if max(width, height) > max_dimension:
            if width > height:
                new_width = max_dimension
                new_height = int(height * (max_dimension / width))
            else:
                new_height = max_dimension
                new_width = int(width * (max_dimension / height))
            
            image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)
            logger.info(f"Image resized from {width}x{height} to {new_width}x{new_height}")
        
        return image
    
    def analyze_layout(self, image_path: str) -> LayoutResult:
        """
        Perform comprehensive layout analysis on image using PP-Structure with retry mechanism
        
        PaddleOCR 3.x API é€‚é…ï¼š
        - ä½¿ç”¨ predict æ–¹æ³•æ›¿ä»£ ocr æ–¹æ³•
        - å¤„ç†æ–°çš„ OCRResult è¿”å›æ ¼å¼
        
        Args:
            image_path: Path to image file
            
        Returns:
            LayoutResult containing detected regions and metadata
        """
        if not self._structure_engine:
            raise OCRProcessingError("Structure engine not initialized")
        
        # Store raw output path for later saving
        self._current_image_path = image_path
        
        @retry_handler.retry(RetryConfig(max_retries=3, base_delay=1.5))
        def perform_layout_analysis():
            try:
                import time
                import json
                import paddleocr
                start_time = time.time()
                
                # Preprocess image for better results and get scale info
                preprocessed_path, scale_info = self.preprocess_image(image_path)
                
                # æ£€æµ‹ PaddleOCR ç‰ˆæœ¬å¹¶ä½¿ç”¨ç›¸åº”çš„ API
                version = getattr(paddleocr, '__version__', '2.0.0')
                is_v3 = version.startswith('3.')
                
                if is_v3:
                    # PaddleOCR 3.x: ä½¿ç”¨ predict æ–¹æ³•
                    # ç¦ç”¨ä¸å¿…è¦çš„åŠŸèƒ½ä»¥åŠ é€Ÿå¤„ç†ï¼š
                    # - use_doc_orientation_classify=False: ç¦ç”¨æ–‡æ¡£æ–¹å‘åˆ†ç±»
                    # - use_doc_unwarping=False: ç¦ç”¨æ–‡æ¡£å»ç•¸å˜
                    # - use_seal_recognition=False: ç¦ç”¨å°ç« è¯†åˆ«
                    # - use_formula_recognition=False: ç¦ç”¨å…¬å¼è¯†åˆ«
                    # - use_chart_recognition=False: ç¦ç”¨å›¾è¡¨è¯†åˆ«
                    raw_result = list(self._structure_engine.predict(
                        preprocessed_path,
                        use_doc_orientation_classify=False,
                        use_doc_unwarping=False,
                        use_seal_recognition=False,
                        use_formula_recognition=False,
                        use_chart_recognition=False
                    ))
                    
                    # å¤„ç† PPStructureV3 çš„è¿”å›æ ¼å¼å¹¶ç¼“å­˜
                    # è¿™æ · extract_tables å¯ä»¥ç›´æ¥ä½¿ç”¨ç¼“å­˜çš„ç»“æœï¼Œé¿å…é‡å¤è°ƒç”¨ predict()
                    processed_ppstructure_result = self._process_ppstructure_v3_result(raw_result, preprocessed_path)
                    self._ppstructure_result_cache[preprocessed_path] = processed_ppstructure_result
                    # åŒæ—¶ç¼“å­˜åŸå§‹å›¾åƒè·¯å¾„çš„ç»“æœ
                    self._ppstructure_result_cache[image_path] = processed_ppstructure_result
                    
                    # ä¿å­˜ PPStructure HTML è¾“å‡ºï¼ˆä¼ å…¥å¼€å§‹æ—¶é—´å’Œ scale_infoï¼‰
                    self._save_ppstructure_html(image_path, processed_ppstructure_result, start_time, scale_info)
                    
                    # ã€ä¿®å¤ã€‘ä½¿ç”¨ PPStructureV3 çš„å¸ƒå±€åˆ†æç»“æœåˆ›å»º regions
                    # è€Œä¸æ˜¯ä½¿ç”¨ OCR æ–‡æœ¬è¡Œç»“æœ
                    regions = self._parse_ppstructure_v3_to_regions(processed_ppstructure_result)
                    
                    # åŒæ—¶ä¿å­˜ OCR æ–‡æœ¬è¡Œç»“æœç”¨äºä¸‹è½½
                    structure_result = self._convert_v3_result_to_legacy(raw_result)
                    self._save_raw_ocr_output(image_path, structure_result, scale_info)
                else:
                    # PaddleOCR 2.x: ä½¿ç”¨ ocr æ–¹æ³•
                    structure_result = self._structure_engine.ocr(preprocessed_path, cls=True)
                    
                    # Save raw OCR output for download
                    self._save_raw_ocr_output(image_path, structure_result, scale_info)
                    
                    # Parse structure results with enhanced classification
                    regions = self._parse_structure_result(structure_result)
                
                # Convert coordinates back to original image scale
                regions = self._scale_regions_to_original(regions, scale_info)
                
                # ã€é‡è¦ã€‘åªåœ¨ PaddleOCR 2.x æ—¶è¿›è¡Œå¯å‘å¼å¸ƒå±€åˆ†ç±»å¢å¼º
                # PaddleOCR 3.x (PPStructureV3) å·²ç»å†…ç½®äº†æ·±åº¦å­¦ä¹ å¸ƒå±€åˆ†æï¼Œ
                # ä¸éœ€è¦ä¹Ÿä¸åº”è¯¥ç”¨å¯å‘å¼è§„åˆ™è¦†ç›–å…¶åˆ†ç±»ç»“æœ
                if not is_v3:
                    regions = self._enhance_layout_classification(regions, image_path)
                
                # Sort regions by reading order (top to bottom, left to right)
                regions = self._sort_regions_by_reading_order(regions)
                
                # Calculate confidence metrics
                confidence_metrics = self._calculate_confidence_metrics(regions)
                
                # è®¡ç®—å¤„ç†æ—¶é—´
                end_time = time.time()
                processing_time = end_time - start_time
                
                # ç”Ÿæˆç½®ä¿¡åº¦è®¡ç®—æ—¥å¿—ï¼ˆåŒ…å«æ—¶é—´ä¿¡æ¯ï¼‰
                try:
                    output_folder = str(Path(image_path).parent)
                    # ä» image_path æå– job_id
                    image_name = Path(image_path).stem
                    if '_page' in image_name:
                        job_id = image_name.split('_page')[0]
                    else:
                        job_id = image_name
                    self.generate_confidence_log(regions, job_id, output_folder, start_time, end_time, processing_time)
                except Exception as e:
                    logger.warning(f"ç”Ÿæˆç½®ä¿¡åº¦æ—¥å¿—å¤±è´¥: {e}")
                
                # ä¿ç•™é¢„å¤„ç†åçš„å›¾åƒç”¨äºè°ƒè¯•ï¼ˆä¸å†åˆ é™¤ï¼‰
                # æ–‡ä»¶åæ ¼å¼: {job_id}_page1_preprocessed.png
                if preprocessed_path != image_path:
                    logger.info(f"ä¿ç•™é¢„å¤„ç†å›¾åƒç”¨äºè°ƒè¯•: {preprocessed_path}")
                
                return LayoutResult(
                    regions=regions,
                    tables=[],  # Tables will be populated in extract_tables method
                    confidence_score=confidence_metrics['overall'],
                    processing_time=processing_time
                )
                
            except Exception as e:
                # Convert certain errors to retryable network errors
                if any(keyword in str(e).lower() for keyword in ['network', 'connection', 'timeout', 'model']):
                    raise NetworkRetryError(f"Layout analysis network error: {e}")
                else:
                    raise OCRProcessingError(f"Layout analysis failed: {e}")
        
        try:
            return perform_layout_analysis()
        except NetworkRetryError as e:
            raise OCRProcessingError(f"Layout analysis failed after retries: {e}")
        except Exception as e:
            raise OCRProcessingError(f"Layout analysis error: {e}")
    
    def _save_raw_ocr_output(self, image_path: str, structure_result: List, scale_info: Dict[str, Any]) -> None:
        """
        Save raw PaddleOCR output for download
        
        Args:
            image_path: Path to the original image
            structure_result: Raw OCR result from PaddleOCR
            scale_info: Scale information from preprocessing
        """
        import json
        from pathlib import Path
        
        try:
            # Extract job_id from image path (format: {job_id}_page1.png)
            image_name = Path(image_path).stem
            if '_page' in image_name:
                job_id = image_name.split('_page')[0]
            else:
                job_id = image_name
            
            # Determine output folder
            output_folder = Path(image_path).parent
            
            # Prepare raw JSON output
            raw_json_data = {
                'job_id': job_id,
                'image_path': str(image_path),
                'scale_info': scale_info,
                'ocr_result': []
            }
            
            # Process OCR results
            if structure_result and len(structure_result) > 0:
                actual_results = structure_result[0] if structure_result else []
                
                for idx, item in enumerate(actual_results):
                    if not item or len(item) < 2:
                        continue
                    
                    try:
                        bbox_coords = item[0]
                        text_info = item[1]
                        
                        # Extract text and confidence
                        if isinstance(text_info, tuple) and len(text_info) >= 2:
                            text_content = text_info[0]
                            confidence = float(text_info[1])
                        else:
                            text_content = str(text_info)
                            confidence = 0.0
                        
                        # Calculate bounding box
                        x_coords = [point[0] for point in bbox_coords]
                        y_coords = [point[1] for point in bbox_coords]
                        bbox = {
                            'x': min(x_coords),
                            'y': min(y_coords),
                            'width': max(x_coords) - min(x_coords),
                            'height': max(y_coords) - min(y_coords),
                            'points': [[float(p[0]), float(p[1])] for p in bbox_coords]
                        }
                        
                        # Add to JSON
                        raw_json_data['ocr_result'].append({
                            'index': idx,
                            'text': text_content,
                            'confidence': confidence,
                            'bbox': bbox
                        })
                        
                    except Exception as e:
                        logger.warning(f"Failed to process OCR item {idx}: {e}")
            
            # Save JSON file
            json_path = output_folder / f"{job_id}_raw_ocr.json"
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(raw_json_data, f, ensure_ascii=False, indent=2)
            logger.info(f"Saved raw OCR JSON to: {json_path}")
            
            # Note: HTML will be saved separately when PPStructure table detection runs
            # The HTML contains the actual table structure from PPStructure
            
        except Exception as e:
            logger.warning(f"Failed to save raw OCR output: {e}")
    
    def _save_ppstructure_html(self, image_path: str, ppstructure_result: List, start_time: float = None, scale_info: Dict[str, Any] = None) -> None:
        """
        Save PPStructure raw HTML output for download - åŒ…å«æ‰€æœ‰å†…å®¹ï¼ˆæ–‡æœ¬+è¡¨æ ¼ï¼‰
        åŒæ—¶è¯»å–æ™®é€š OCR ç»“æœï¼Œå°†æœªè¢« PPStructure è¯†åˆ«çš„æ–‡æœ¬ä¹Ÿæ·»åŠ åˆ° HTML ä¸­
        
        Args:
            image_path: Path to the original image
            ppstructure_result: Raw result from PPStructure
            start_time: OCR å¤„ç†å¼€å§‹æ—¶é—´æˆ³
            scale_info: å›¾åƒç¼©æ”¾ä¿¡æ¯
        """
        from pathlib import Path
        import json
        import time
        from datetime import datetime
        
        try:
            # Extract job_id from image path
            image_name = Path(image_path).stem
            if '_page' in image_name:
                job_id = image_name.split('_page')[0]
            else:
                job_id = image_name
            
            output_folder = Path(image_path).parent
            
            # è®¡ç®—æ—¶é—´ä¿¡æ¯
            current_time = time.time()
            start_datetime = datetime.fromtimestamp(start_time).strftime('%Y-%m-%d %H:%M:%S') if start_time else None
            current_datetime = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            elapsed_time = f"{current_time - start_time:.2f}s" if start_time else None
            
            # ä¿å­˜ PPStructure åŸå§‹ç»“æœåˆ° JSON æ–‡ä»¶
            ppstructure_json_path = output_folder / f"{job_id}_ppstructure.json"
            
            # ä½¿ç”¨ä¼ å…¥çš„ scale_info
            scale_info_for_save = scale_info if scale_info else {}
            
            ppstructure_json_data = {
                'job_id': job_id,
                'image_path': str(image_path),
                'processing_info': {
                    'start_time': start_datetime,
                    'save_time': current_datetime,
                    'elapsed_at_save': elapsed_time
                },
                'scale_info': scale_info_for_save,  # æ·»åŠ ç¼©æ”¾ä¿¡æ¯ç”¨äºè°ƒè¯•
                'total_items': len(ppstructure_result),
                'items': []
            }
            
            for idx, item in enumerate(ppstructure_result):
                item_data = {
                    'index': idx,
                    'type': item.get('type', 'unknown'),
                    'bbox': item.get('bbox', []),
                    'res': None
                }
                
                # å¤„ç† res å­—æ®µ
                res = item.get('res', {})
                if isinstance(res, dict):
                    # è¡¨æ ¼ç±»å‹ï¼ŒåŒ…å« html å’Œ cell_bbox
                    # è¡¨æ ¼æ—  OCR ç½®ä¿¡åº¦ï¼ˆSLANet æ¨¡å‹ä¸è¾“å‡ºç½®ä¿¡åº¦ï¼‰
                    item_data['res'] = {
                        'html': res.get('html', ''),
                        'cell_bbox': res.get('cell_bbox', []),
                        'confidence': res.get('confidence', None)  # è¡¨æ ¼ç½®ä¿¡åº¦ä¸º None
                    }
                elif isinstance(res, list):
                    # æ–‡æœ¬ç±»å‹ï¼ŒåŒ…å«æ–‡æœ¬è¡Œåˆ—è¡¨
                    item_data['res'] = []
                    for text_item in res:
                        if isinstance(text_item, dict):
                            # ä¿ç•™çœŸå®çš„ç½®ä¿¡åº¦ï¼ŒNone è¡¨ç¤ºæ— ç½®ä¿¡åº¦
                            conf = text_item.get('confidence')
                            item_data['res'].append({
                                'text': text_item.get('text', ''),
                                'confidence': conf,  # ä¿ç•™ None æˆ–çœŸå®å€¼
                                'text_region': text_item.get('text_region', [])
                            })
                elif isinstance(res, str):
                    item_data['res'] = res
                
                ppstructure_json_data['items'].append(item_data)
            
            with open(ppstructure_json_path, 'w', encoding='utf-8') as f:
                json.dump(ppstructure_json_data, f, ensure_ascii=False, indent=2)
            logger.info(f"Saved PPStructure JSON to: {ppstructure_json_path}")
            
            # è¯»å–æ™®é€š OCR ç»“æœï¼ˆåŒ…å«æ‰€æœ‰æ–‡æœ¬è¡Œï¼‰
            ocr_json_path = output_folder / f"{job_id}_raw_ocr.json"
            ocr_text_items = []
            scale_info = {}
            if ocr_json_path.exists():
                with open(ocr_json_path, 'r', encoding='utf-8') as f:
                    ocr_data = json.load(f)
                    ocr_text_items = ocr_data.get('ocr_result', [])
                    scale_info = ocr_data.get('scale_info', {})
                logger.info(f"Loaded {len(ocr_text_items)} text items from OCR JSON")
            
            # æŒ‰ y åæ ‡æ’åºæ‰€æœ‰ PPStructure åŒºåŸŸ
            sorted_items = sorted(ppstructure_result, key=lambda x: x.get('bbox', [0, 0, 0, 0])[1])
            
            # å»é‡ï¼šè¿‡æ»¤æ‰é‡å ä¸”å†…å®¹ç›¸åŒçš„åŒºåŸŸï¼ˆä¿ç•™ç¬¬ä¸€ä¸ªï¼‰
            def boxes_overlap(bbox1, bbox2, threshold=0.7):
                """æ£€æŸ¥ä¸¤ä¸ªè¾¹ç•Œæ¡†æ˜¯å¦é‡å è¶…è¿‡é˜ˆå€¼"""
                x1 = max(bbox1[0], bbox2[0])
                y1 = max(bbox1[1], bbox2[1])
                x2 = min(bbox1[2], bbox2[2])
                y2 = min(bbox1[3], bbox2[3])
                
                if x1 >= x2 or y1 >= y2:
                    return False
                
                intersection = (x2 - x1) * (y2 - y1)
                area1 = (bbox1[2] - bbox1[0]) * (bbox1[3] - bbox1[1])
                area2 = (bbox2[2] - bbox2[0]) * (bbox2[3] - bbox2[1])
                min_area = min(area1, area2)
                
                return intersection / min_area > threshold if min_area > 0 else False
            
            # è¿‡æ»¤é‡å ä¸”å†…å®¹ç›¸åŒçš„åŒºåŸŸ
            filtered_items = []
            for item in sorted_items:
                bbox = item.get('bbox', [0, 0, 0, 0])
                item_text = self._extract_text_from_res(item.get('res', {}))
                is_duplicate = False
                for existing in filtered_items:
                    existing_bbox = existing.get('bbox', [0, 0, 0, 0])
                    if boxes_overlap(bbox, existing_bbox):
                        # åªæœ‰å½“æ–‡æœ¬å†…å®¹ä¹Ÿç›¸åŒæ—¶æ‰è®¤ä¸ºæ˜¯é‡å¤
                        existing_text = self._extract_text_from_res(existing.get('res', {}))
                        if item_text == existing_text or not item_text:
                            is_duplicate = True
                            logger.info(f"Filtered duplicate region: {item.get('type')} overlaps with {existing.get('type')}, same content")
                            break
                if not is_duplicate:
                    filtered_items.append(item)
            
            sorted_items = filtered_items
            
            # ç»Ÿè®¡å„ç±»å‹æ•°é‡
            type_counts = {}
            for item in sorted_items:
                item_type = item.get('type', 'unknown')
                type_counts[item_type] = type_counts.get(item_type, 0) + 1
            logger.info(f"PPStructure result types (after dedup): {type_counts}")
            
            # è·å–æ‰€æœ‰ PPStructure åŒºåŸŸçš„è¾¹ç•Œæ¡†ï¼ˆç”¨äºè¿‡æ»¤é‡å¤æ–‡æœ¬ï¼‰
            ppstructure_bboxes = []
            for item in sorted_items:
                bbox = item.get('bbox', [0, 0, 0, 0])
                ppstructure_bboxes.append({
                    'x1': bbox[0], 'y1': bbox[1], 'x2': bbox[2], 'y2': bbox[3],
                    'type': item.get('type', 'unknown')
                })
            
            # è¿‡æ»¤å‡ºä¸åœ¨ PPStructure åŒºåŸŸå†…çš„æ–‡æœ¬ï¼ˆé¿å…é‡å¤ï¼‰
            # åæ ‡éœ€è¦æ ¹æ® scale_info è½¬æ¢
            scale_x = scale_info.get('scale_x', 1.0)
            scale_y = scale_info.get('scale_y', 1.0)
            
            standalone_texts = []
            for text_item in ocr_text_items:
                bbox = text_item.get('bbox', {})
                # è½¬æ¢åæ ‡åˆ°åŸå§‹å›¾åƒå°ºå¯¸
                text_x = bbox.get('x', 0) * scale_x
                text_y = bbox.get('y', 0) * scale_y
                text_x2 = text_x + bbox.get('width', 0) * scale_x
                text_y2 = text_y + bbox.get('height', 0) * scale_y
                text_center_x = (text_x + text_x2) / 2
                text_center_y = (text_y + text_y2) / 2
                
                # æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åœ¨ä»»ä½• PPStructure åŒºåŸŸå†…
                is_inside_ppstructure = False
                for pp_bbox in ppstructure_bboxes:
                    # å¦‚æœæ–‡æœ¬ä¸­å¿ƒç‚¹åœ¨ PPStructure åŒºåŸŸå†…ï¼Œåˆ™è®¤ä¸ºæ˜¯é‡å¤çš„
                    if (pp_bbox['x1'] <= text_center_x <= pp_bbox['x2'] and
                        pp_bbox['y1'] <= text_center_y <= pp_bbox['y2']):
                        is_inside_ppstructure = True
                        break
                
                if not is_inside_ppstructure:
                    standalone_texts.append({
                        'text': text_item.get('text', ''),
                        'confidence': text_item.get('confidence', 0),
                        'bbox': {
                            'x': text_x, 'y': text_y,
                            'x2': text_x2, 'y2': text_y2
                        },
                        'original_bbox': bbox
                    })
            
            logger.info(f"Found {len(standalone_texts)} standalone text items not in PPStructure regions")
            
            # è°ƒè¯•ï¼šæ£€æŸ¥ç‰¹å®šæ–‡æœ¬
            for text_item in ocr_text_items:
                if 'DOMESTIC' in text_item.get('text', ''):
                    bbox = text_item.get('bbox', {})
                    text_x = bbox.get('x', 0) * scale_x
                    text_y = bbox.get('y', 0) * scale_y
                    text_x2 = text_x + bbox.get('width', 0) * scale_x
                    text_y2 = text_y + bbox.get('height', 0) * scale_y
                    text_center_x = (text_x + text_x2) / 2
                    text_center_y = (text_y + text_y2) / 2
                    logger.info(f"DEBUG DOMESTIC: text='{text_item.get('text')}', center=({text_center_x:.1f}, {text_center_y:.1f})")
                    for pp_bbox in ppstructure_bboxes:
                        if (pp_bbox['x1'] <= text_center_x <= pp_bbox['x2'] and
                            pp_bbox['y1'] <= text_center_y <= pp_bbox['y2']):
                            logger.info(f"DEBUG DOMESTIC: INSIDE {pp_bbox['type']} region ({pp_bbox['x1']:.0f},{pp_bbox['y1']:.0f})-({pp_bbox['x2']:.0f},{pp_bbox['y2']:.0f})")
            
            # åˆå¹¶ PPStructure åŒºåŸŸå’Œç‹¬ç«‹æ–‡æœ¬ï¼ŒæŒ‰ y åæ ‡æ’åº
            all_items = []
            
            # æ·»åŠ  PPStructure åŒºåŸŸ
            for idx, item in enumerate(sorted_items):
                bbox = item.get('bbox', [0, 0, 0, 0])
                all_items.append({
                    'source': 'ppstructure',
                    'type': item.get('type', 'unknown'),
                    'data': item,
                    'y': bbox[1],
                    'idx': idx
                })
            
            # æ·»åŠ ç‹¬ç«‹æ–‡æœ¬
            for idx, text_item in enumerate(standalone_texts):
                all_items.append({
                    'source': 'ocr',
                    'type': 'text',
                    'data': text_item,
                    'y': text_item['bbox']['y'],
                    'idx': len(sorted_items) + idx
                })
            
            logger.info(f"all_items count: {len(all_items)} (PPStructure: {len(sorted_items)}, standalone: {len(standalone_texts)})")
            
            # æŒ‰ y åæ ‡æ’åº
            all_items.sort(key=lambda x: x['y'])
            
            # Build HTML document with all content
            html_parts = []
            html_parts.append('<!DOCTYPE html>')
            html_parts.append('<html lang="zh-CN">')
            html_parts.append('<head>')
            html_parts.append('<meta charset="UTF-8">')
            html_parts.append('<meta name="viewport" content="width=device-width, initial-scale=1.0">')
            html_parts.append(f'<title>OCRè¯†åˆ«ç»“æœ - {job_id}</title>')
            html_parts.append('<style>')
            html_parts.append('''
body { 
    font-family: "Microsoft YaHei", "SimSun", Arial, sans-serif; 
    max-width: 900px; 
    margin: 0 auto; 
    padding: 20px;
    line-height: 1.6;
    color: #333;
}
.ocr-region {
    cursor: pointer;
    transition: all 0.2s ease;
    border-radius: 3px;
    position: relative;
    padding: 8px;
    margin: 10px 0;
}
.ocr-region:hover {
    background-color: rgba(66, 133, 244, 0.1);
    outline: 2px solid rgba(66, 133, 244, 0.3);
}
.ocr-region.title {
    font-size: 1.4em;
    font-weight: bold;
    color: #1a1a1a;
    margin: 20px 0 10px 0;
}
.ocr-region.text-block {
    line-height: 1.8;
}
.ocr-region.header, .ocr-region.footer {
    font-size: 0.9em;
    color: #666;
}
.ocr-region.figure-caption, .ocr-region.table-caption {
    font-size: 0.9em;
    color: #666;
    text-align: center;
    font-style: italic;
}
.ocr-region.reference {
    font-size: 0.85em;
    color: #555;
}
.ocr-region.figure-placeholder {
    background: #f5f5f5;
    border: 1px dashed #ccc;
    text-align: center;
    color: #888;
    padding: 30px;
    margin: 15px 0;
}
.table-wrapper {
    margin: 15px 0;
    overflow-x: auto;
}
table { 
    border-collapse: collapse; 
    width: 100%; 
    font-size: 0.95em;
}
table td, table th { 
    border: 1px solid #ccc; 
    padding: 8px 12px; 
    text-align: left;
    vertical-align: top;
}
table th { 
    background: #f5f5f5; 
    font-weight: bold;
}
table tr:nth-child(even) {
    background: #fafafa;
}
.no-content {
    text-align: center;
    color: #888;
    padding: 40px;
    font-size: 1.1em;
}
.editable-content {
    display: block;
}
''')
            html_parts.append('</style>')
            html_parts.append('</head>')
            html_parts.append('<body>')
            
            if not all_items:
                html_parts.append('<div class="no-content">ğŸ“‹ æœªæ£€æµ‹åˆ°å†…å®¹</div>')
            else:
                # Process all items in reading order
                for item_wrapper in all_items:
                    idx = item_wrapper['idx']
                    source = item_wrapper['source']
                    item_type = item_wrapper['type']
                    
                    if source == 'ocr':
                        # æ¥è‡ªæ™®é€š OCR çš„ç‹¬ç«‹æ–‡æœ¬
                        text_data = item_wrapper['data']
                        text_content = text_data.get('text', '')
                        confidence = text_data.get('confidence', 0)
                        bbox = text_data.get('bbox', {})
                        bbox_data = json.dumps({'x': float(bbox.get('x', 0)), 'y': float(bbox.get('y', 0)), 'x2': float(bbox.get('x2', 0)), 'y2': float(bbox.get('y2', 0))})
                        
                        if text_content:
                            html_parts.append(f'<div class="ocr-region text-block" data-region-id="{idx}" data-region-type="text" data-bbox=\'{bbox_data}\' data-confidence="{confidence:.2f}">')
                            html_parts.append(f'<span class="editable-content">{text_content}</span>')
                            html_parts.append('</div>')
                    else:
                        # æ¥è‡ª PPStructure çš„åŒºåŸŸ
                        item = item_wrapper['data']
                        res = item.get('res', {})
                        bbox = item.get('bbox', [0, 0, 0, 0])
                        bbox_data = json.dumps({'x': float(bbox[0]), 'y': float(bbox[1]), 'x2': float(bbox[2]), 'y2': float(bbox[3])})
                        
                        if item_type == 'table':
                            # è¡¨æ ¼ï¼šä½¿ç”¨ PPStructure è¿”å›çš„ HTML
                            if isinstance(res, dict) and 'html' in res:
                                table_html = res['html']
                                table_html = table_html.replace('<html>', '').replace('</html>', '')
                                table_html = table_html.replace('<body>', '').replace('</body>', '')
                                html_parts.append(f'<div class="ocr-region table-wrapper" data-region-id="{idx}" data-region-type="table" data-bbox=\'{bbox_data}\'>')
                                html_parts.append(table_html.strip())
                                html_parts.append('</div>')
                            elif isinstance(res, str):
                                html_parts.append(f'<div class="ocr-region table-wrapper" data-region-id="{idx}" data-region-type="table" data-bbox=\'{bbox_data}\'>')
                                html_parts.append(res)
                                html_parts.append('</div>')
                        
                        elif item_type == 'title':
                            # æ ‡é¢˜
                            text_content = self._extract_text_from_res(res)
                            if text_content:
                                html_parts.append(f'<div class="ocr-region title" data-region-id="{idx}" data-region-type="text" data-bbox=\'{bbox_data}\'>')
                                html_parts.append(f'<span class="editable-content">{text_content}</span>')
                                html_parts.append('</div>')
                        
                        elif item_type == 'text':
                            # æ™®é€šæ–‡æœ¬
                            text_content = self._extract_text_from_res(res)
                            if text_content:
                                html_parts.append(f'<div class="ocr-region text-block" data-region-id="{idx}" data-region-type="text" data-bbox=\'{bbox_data}\'>')
                                html_parts.append(f'<span class="editable-content">{text_content}</span>')
                                html_parts.append('</div>')
                        
                        elif item_type == 'figure':
                            # å›¾åƒåŒºåŸŸï¼šå°è¯•ä»æ™®é€š OCR ç»“æœä¸­æå–è¯¥åŒºåŸŸå†…çš„æ–‡æœ¬
                            figure_bbox = item.get('bbox', [0, 0, 0, 0])
                            figure_texts = []
                            for text_item in ocr_text_items:
                                t_bbox = text_item.get('bbox', {})
                                # è½¬æ¢åæ ‡åˆ°åŸå§‹å›¾åƒå°ºå¯¸
                                t_x = t_bbox.get('x', 0) * scale_x
                                t_y = t_bbox.get('y', 0) * scale_y
                                t_x2 = t_x + t_bbox.get('width', 0) * scale_x
                                t_y2 = t_y + t_bbox.get('height', 0) * scale_y
                                t_center_x = (t_x + t_x2) / 2
                                t_center_y = (t_y + t_y2) / 2
                                
                                # æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åœ¨ figure åŒºåŸŸå†…
                                if (figure_bbox[0] <= t_center_x <= figure_bbox[2] and
                                    figure_bbox[1] <= t_center_y <= figure_bbox[3]):
                                    figure_texts.append({
                                        'text': text_item.get('text', ''),
                                        'y': t_y,
                                        'x': t_x
                                    })
                            
                            if figure_texts:
                                # æŒ‰ y åæ ‡æ’åºï¼Œç„¶åæŒ‰ x åæ ‡
                                figure_texts.sort(key=lambda x: (x['y'], x['x']))
                                combined_text = ' '.join([t['text'] for t in figure_texts])
                                html_parts.append(f'<div class="ocr-region text-block" data-region-id="{idx}" data-region-type="text" data-bbox=\'{bbox_data}\'>')
                                html_parts.append(f'<span class="editable-content">{combined_text}</span>')
                                html_parts.append('</div>')
                            else:
                                # æ²¡æœ‰æ–‡æœ¬ï¼Œæ˜¾ç¤ºå›¾åƒå ä½ç¬¦
                                html_parts.append(f'<div class="ocr-region figure-placeholder" data-region-id="{idx}" data-region-type="figure" data-bbox=\'{bbox_data}\'>[å›¾åƒåŒºåŸŸ]</div>')
                        
                        elif item_type == 'figure_caption':
                            # å›¾åƒè¯´æ˜
                            text_content = self._extract_text_from_res(res)
                            if text_content:
                                html_parts.append(f'<div class="ocr-region figure-caption" data-region-id="{idx}" data-region-type="text" data-bbox=\'{bbox_data}\'>')
                                html_parts.append(f'<span class="editable-content">{text_content}</span>')
                                html_parts.append('</div>')
                        
                        elif item_type == 'table_caption':
                            # è¡¨æ ¼è¯´æ˜
                            text_content = self._extract_text_from_res(res)
                            if text_content:
                                html_parts.append(f'<div class="ocr-region table-caption" data-region-id="{idx}" data-region-type="text" data-bbox=\'{bbox_data}\'>')
                                html_parts.append(f'<span class="editable-content">{text_content}</span>')
                                html_parts.append('</div>')
                        
                        elif item_type == 'header':
                            # é¡µçœ‰
                            text_content = self._extract_text_from_res(res)
                            if text_content:
                                html_parts.append(f'<div class="ocr-region header" data-region-id="{idx}" data-region-type="text" data-bbox=\'{bbox_data}\'>')
                                html_parts.append(f'<span class="editable-content">{text_content}</span>')
                                html_parts.append('</div>')
                        
                        elif item_type == 'footer':
                            # é¡µè„š
                            text_content = self._extract_text_from_res(res)
                            if text_content:
                                html_parts.append(f'<div class="ocr-region footer" data-region-id="{idx}" data-region-type="text" data-bbox=\'{bbox_data}\'>')
                                html_parts.append(f'<span class="editable-content">{text_content}</span>')
                                html_parts.append('</div>')
                        
                        elif item_type == 'reference':
                            # å‚è€ƒæ–‡çŒ®
                            text_content = self._extract_text_from_res(res)
                            if text_content:
                                html_parts.append(f'<div class="ocr-region reference" data-region-id="{idx}" data-region-type="text" data-bbox=\'{bbox_data}\'>')
                                html_parts.append(f'<span class="editable-content">{text_content}</span>')
                                html_parts.append('</div>')
                        
                        elif item_type == 'equation':
                            # å…¬å¼
                            text_content = self._extract_text_from_res(res)
                            if text_content:
                                html_parts.append(f'<div class="ocr-region equation" data-region-id="{idx}" data-region-type="text" data-bbox=\'{bbox_data}\'>')
                                html_parts.append(f'<span class="editable-content"><em>{text_content}</em></span>')
                                html_parts.append('</div>')
                        
                        else:
                            # å…¶ä»–ç±»å‹ï¼Œå°è¯•æå–æ–‡æœ¬
                            text_content = self._extract_text_from_res(res)
                            if text_content:
                                html_parts.append(f'<div class="ocr-region text-block" data-region-id="{idx}" data-region-type="text" data-bbox=\'{bbox_data}\'>')
                                html_parts.append(f'<span class="editable-content">{text_content}</span>')
                                html_parts.append('</div>')
            
            html_parts.append('</body>')
            html_parts.append('</html>')
            
            # Save HTML file
            html_content = '\n'.join(html_parts)
            html_path = output_folder / f"{job_id}_raw_ocr.html"
            with open(html_path, 'w', encoding='utf-8') as f:
                f.write(html_content)
            logger.info(f"Saved full HTML to: {html_path} with {len(all_items)} items (PPStructure: {len(sorted_items)}, standalone OCR text: {len(standalone_texts)})")
            
        except Exception as e:
            logger.warning(f"Failed to save PPStructure HTML output: {e}")
            import traceback
            logger.warning(traceback.format_exc())
    
    def _extract_text_from_res(self, res) -> str:
        """
        Extract text content from PPStructure result 'res' field
        
        Args:
            res: The 'res' field from PPStructure result item
            
        Returns:
            Extracted text as a string
        """
        if isinstance(res, str):
            return res
        
        if isinstance(res, list):
            text_lines = []
            for item in res:
                if isinstance(item, dict):
                    # Format: {'text': ..., 'confidence': ..., 'text_region': ...}
                    if 'text' in item:
                        text_lines.append(str(item['text']))
                elif isinstance(item, (list, tuple)) and len(item) >= 2:
                    # Format: [bbox, (text, confidence)] or [bbox, text]
                    text_info = item[1]
                    if isinstance(text_info, (list, tuple)) and len(text_info) >= 1:
                        text_lines.append(str(text_info[0]))
                    else:
                        text_lines.append(str(text_info))
                elif isinstance(item, str):
                    text_lines.append(item)
            return ' '.join(text_lines)
        
        if isinstance(res, dict):
            # Try common keys
            if 'text' in res:
                return str(res['text'])
            if 'html' in res:
                # Strip HTML tags for plain text
                import re
                return re.sub(r'<[^>]+>', '', res['html'])
        
        return str(res) if res else ''
    
    def generate_editable_html(self, image_path: str, ppstructure_result: List) -> str:
        """
        Generate editable HTML content for frontend rendering
        
        This method generates HTML with data attributes for inline editing.
        Each region has data-region-id, data-region-type, and data-bbox attributes.
        
        Args:
            image_path: Path to the original image
            ppstructure_result: Raw result from PPStructure
            
        Returns:
            HTML string for frontend rendering
        """
        import json
        from pathlib import Path
        
        # Extract job_id from image path
        image_name = Path(image_path).stem
        if '_page' in image_name:
            job_id = image_name.split('_page')[0]
        else:
            job_id = image_name
        
        # Sort results by y-coordinate (top to bottom reading order)
        sorted_results = sorted(ppstructure_result, key=lambda x: x.get('bbox', [0, 0, 0, 0])[1])
        
        html_parts = []
        
        # Process each item from PPStructure result (sorted by position)
        for idx, item in enumerate(sorted_results):
            item_type = item.get('type', 'unknown')
            res = item.get('res', {})
            bbox = item.get('bbox', [0, 0, 0, 0])
            
            # Create bbox JSON for data attribute
            bbox_data = json.dumps({'x': float(bbox[0]), 'y': float(bbox[1]), 'x2': float(bbox[2]), 'y2': float(bbox[3])})
            
            # Handle different content types
            if item_type == 'table':
                if isinstance(res, dict) and 'html' in res:
                    table_html = res['html']
                    table_html = table_html.replace('<html>', '').replace('</html>', '')
                    table_html = table_html.replace('<body>', '').replace('</body>', '')
                    html_parts.append(f'<div class="ocr-region table-wrapper" data-region-id="{idx}" data-region-type="table" data-bbox=\'{bbox_data}\'>')
                    html_parts.append(table_html.strip())
                    html_parts.append('</div>')
                elif isinstance(res, str):
                    html_parts.append(f'<div class="ocr-region table-wrapper" data-region-id="{idx}" data-region-type="table" data-bbox=\'{bbox_data}\'>')
                    html_parts.append(res)
                    html_parts.append('</div>')
                    
            elif item_type == 'title':
                text_content = self._extract_text_from_res(res)
                if text_content:
                    html_parts.append(f'<div class="ocr-region title" data-region-id="{idx}" data-region-type="text" data-bbox=\'{bbox_data}\'>')
                    html_parts.append(f'<span class="editable-content">{text_content}</span>')
                    html_parts.append('</div>')
                    
            elif item_type == 'text':
                text_content = self._extract_text_from_res(res)
                if text_content:
                    html_parts.append(f'<div class="ocr-region text-block" data-region-id="{idx}" data-region-type="text" data-bbox=\'{bbox_data}\'>')
                    html_parts.append(f'<span class="editable-content">{text_content}</span>')
                    html_parts.append('</div>')
                    
            elif item_type == 'header':
                text_content = self._extract_text_from_res(res)
                if text_content:
                    html_parts.append(f'<div class="ocr-region header" data-region-id="{idx}" data-region-type="text" data-bbox=\'{bbox_data}\'>')
                    html_parts.append(f'<span class="editable-content">{text_content}</span>')
                    html_parts.append('</div>')
                    
            elif item_type == 'footer':
                text_content = self._extract_text_from_res(res)
                if text_content:
                    html_parts.append(f'<div class="ocr-region footer" data-region-id="{idx}" data-region-type="text" data-bbox=\'{bbox_data}\'>')
                    html_parts.append(f'<span class="editable-content">{text_content}</span>')
                    html_parts.append('</div>')
                    
            elif item_type == 'figure':
                html_parts.append(f'<div class="ocr-region figure-placeholder" data-region-id="{idx}" data-region-type="figure" data-bbox=\'{bbox_data}\'>[å›¾åƒ]</div>')
                
            elif item_type == 'figure_caption':
                text_content = self._extract_text_from_res(res)
                if text_content:
                    html_parts.append(f'<div class="ocr-region figure-caption" data-region-id="{idx}" data-region-type="text" data-bbox=\'{bbox_data}\'>')
                    html_parts.append(f'<span class="editable-content">{text_content}</span>')
                    html_parts.append('</div>')
                    
            elif item_type == 'table_caption':
                text_content = self._extract_text_from_res(res)
                if text_content:
                    html_parts.append(f'<div class="ocr-region table-caption" data-region-id="{idx}" data-region-type="text" data-bbox=\'{bbox_data}\'>')
                    html_parts.append(f'<span class="editable-content">{text_content}</span>')
                    html_parts.append('</div>')
                    
            elif item_type == 'reference':
                text_content = self._extract_text_from_res(res)
                if text_content:
                    html_parts.append(f'<div class="ocr-region reference" data-region-id="{idx}" data-region-type="text" data-bbox=\'{bbox_data}\'>')
                    html_parts.append(f'<span class="editable-content">{text_content}</span>')
                    html_parts.append('</div>')
                    
            elif item_type == 'equation':
                text_content = self._extract_text_from_res(res)
                if text_content:
                    html_parts.append(f'<div class="ocr-region equation" data-region-id="{idx}" data-region-type="text" data-bbox=\'{bbox_data}\'>')
                    html_parts.append(f'<span class="editable-content"><em>{text_content}</em></span>')
                    html_parts.append('</div>')
                    
            else:
                text_content = self._extract_text_from_res(res)
                if text_content:
                    html_parts.append(f'<div class="ocr-region text-block" data-region-id="{idx}" data-region-type="text" data-bbox=\'{bbox_data}\'>')
                    html_parts.append(f'<span class="editable-content">{text_content}</span>')
                    html_parts.append('</div>')
        
        return '\n'.join(html_parts)
    
    def generate_markdown_output(self, image_path: str, ppstructure_result: List) -> str:
        """
        Generate Markdown output from PPStructure result
        
        PaddleOCR 3.x æ–°åŠŸèƒ½ï¼šæ”¯æŒ Markdown æ ¼å¼è¾“å‡º
        
        Args:
            image_path: Path to the original image
            ppstructure_result: Raw result from PPStructure
            
        Returns:
            Markdown formatted string
        """
        from pathlib import Path
        
        # Extract job_id from image path
        image_name = Path(image_path).stem
        if '_page' in image_name:
            job_id = image_name.split('_page')[0]
        else:
            job_id = image_name
        
        # Sort results by y-coordinate (top to bottom reading order)
        sorted_results = sorted(ppstructure_result, key=lambda x: x.get('bbox', [0, 0, 0, 0])[1])
        
        markdown_parts = []
        
        for item in sorted_results:
            item_type = item.get('type', 'unknown')
            res = item.get('res', {})
            
            if item_type == 'title':
                text_content = self._extract_text_from_res(res)
                if text_content:
                    # ä½¿ç”¨ # ä½œä¸ºæ ‡é¢˜
                    markdown_parts.append(f"# {text_content}")
                    markdown_parts.append("")
                    
            elif item_type == 'text':
                text_content = self._extract_text_from_res(res)
                if text_content:
                    markdown_parts.append(text_content)
                    markdown_parts.append("")
                    
            elif item_type == 'header':
                text_content = self._extract_text_from_res(res)
                if text_content:
                    # é¡µçœ‰ä½¿ç”¨æ–œä½“
                    markdown_parts.append(f"*{text_content}*")
                    markdown_parts.append("")
                    
            elif item_type == 'footer':
                text_content = self._extract_text_from_res(res)
                if text_content:
                    # é¡µè„šä½¿ç”¨æ–œä½“
                    markdown_parts.append(f"*{text_content}*")
                    markdown_parts.append("")
                    
            elif item_type == 'table':
                table_md = self._convert_table_to_markdown(res)
                if table_md:
                    markdown_parts.append(table_md)
                    markdown_parts.append("")
                    
            elif item_type == 'figure':
                markdown_parts.append("![å›¾åƒ]()")
                markdown_parts.append("")
                
            elif item_type == 'figure_caption':
                text_content = self._extract_text_from_res(res)
                if text_content:
                    markdown_parts.append(f"*å›¾: {text_content}*")
                    markdown_parts.append("")
                    
            elif item_type == 'table_caption':
                text_content = self._extract_text_from_res(res)
                if text_content:
                    markdown_parts.append(f"*è¡¨: {text_content}*")
                    markdown_parts.append("")
                    
            elif item_type == 'reference':
                text_content = self._extract_text_from_res(res)
                if text_content:
                    markdown_parts.append(f"> {text_content}")
                    markdown_parts.append("")
                    
            elif item_type == 'equation':
                text_content = self._extract_text_from_res(res)
                if text_content:
                    # å…¬å¼ä½¿ç”¨ LaTeX æ ¼å¼
                    markdown_parts.append(f"$${text_content}$$")
                    markdown_parts.append("")
                    
            else:
                text_content = self._extract_text_from_res(res)
                if text_content:
                    markdown_parts.append(text_content)
                    markdown_parts.append("")
        
        markdown_content = '\n'.join(markdown_parts)
        
        # ä¿å­˜ Markdown æ–‡ä»¶
        output_folder = Path(image_path).parent
        md_path = output_folder / f"{job_id}_raw_ocr.md"
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        logger.info(f"Saved Markdown to: {md_path}")
        
        return markdown_content
    
    def _convert_table_to_markdown(self, table_res) -> str:
        """
        Convert table result to Markdown table format
        
        Args:
            table_res: Table result from PPStructure (dict with 'html' or list)
            
        Returns:
            Markdown table string
        """
        try:
            # å¦‚æœæœ‰ HTMLï¼Œä» HTML è§£æ
            if isinstance(table_res, dict) and 'html' in table_res:
                return self._html_table_to_markdown(table_res['html'])
            
            # å¦‚æœæ˜¯åˆ—è¡¨æ ¼å¼
            if isinstance(table_res, list):
                return self._list_to_markdown_table(table_res)
            
            return ""
            
        except Exception as e:
            logger.warning(f"Failed to convert table to Markdown: {e}")
            return ""
    
    def _html_table_to_markdown(self, html_content: str) -> str:
        """
        Convert HTML table to Markdown format
        
        Args:
            html_content: HTML table string
            
        Returns:
            Markdown table string
        """
        try:
            from bs4 import BeautifulSoup
            
            soup = BeautifulSoup(html_content, 'html.parser')
            table = soup.find('table')
            
            if not table:
                return ""
            
            rows = table.find_all('tr')
            if not rows:
                return ""
            
            markdown_rows = []
            max_cols = 0
            
            for row_idx, row in enumerate(rows):
                cells = row.find_all(['td', 'th'])
                cell_texts = [cell.get_text(strip=True) for cell in cells]
                max_cols = max(max_cols, len(cell_texts))
                
                # è½¬ä¹‰ Markdown ç‰¹æ®Šå­—ç¬¦
                cell_texts = [text.replace('|', '\\|') for text in cell_texts]
                
                markdown_rows.append('| ' + ' | '.join(cell_texts) + ' |')
                
                # åœ¨ç¬¬ä¸€è¡Œåæ·»åŠ åˆ†éš”ç¬¦
                if row_idx == 0:
                    separator = '| ' + ' | '.join(['---'] * len(cell_texts)) + ' |'
                    markdown_rows.append(separator)
            
            return '\n'.join(markdown_rows)
            
        except ImportError:
            logger.warning("BeautifulSoup not available for HTML table parsing")
            return ""
        except Exception as e:
            logger.warning(f"HTML table to Markdown conversion failed: {e}")
            return ""
    
    def _list_to_markdown_table(self, table_data: List) -> str:
        """
        Convert list-based table data to Markdown format
        
        Args:
            table_data: List of rows, each row is a list of cells
            
        Returns:
            Markdown table string
        """
        if not table_data:
            return ""
        
        markdown_rows = []
        
        for row_idx, row in enumerate(table_data):
            if isinstance(row, list):
                cell_texts = [str(cell).replace('|', '\\|') for cell in row]
                markdown_rows.append('| ' + ' | '.join(cell_texts) + ' |')
                
                # åœ¨ç¬¬ä¸€è¡Œåæ·»åŠ åˆ†éš”ç¬¦
                if row_idx == 0:
                    separator = '| ' + ' | '.join(['---'] * len(cell_texts)) + ' |'
                    markdown_rows.append(separator)
        
        return '\n'.join(markdown_rows)
    
    def _scale_regions_to_original(self, regions: List[Region], scale_info: Dict[str, Any]) -> List[Region]:
        """
        Scale region coordinates from preprocessed image back to original image dimensions
        
        Args:
            regions: List of regions with coordinates from preprocessed image
            scale_info: Dictionary containing scale factors and dimensions
            
        Returns:
            List of regions with coordinates scaled to original image
        """
        if not scale_info.get('was_resized', False):
            # No scaling needed if image wasn't resized
            logger.info("No coordinate scaling needed - image was not resized")
            return regions
        
        scale_x = scale_info.get('scale_x', 1.0)
        scale_y = scale_info.get('scale_y', 1.0)
        
        logger.info(f"Scaling coordinates by {scale_x:.3f}x{scale_y:.3f} to match original image")
        
        scaled_regions = []
        for region in regions:
            # Scale the bounding box coordinates
            scaled_bbox = BoundingBox(
                x=region.coordinates.x * scale_x,
                y=region.coordinates.y * scale_y,
                width=region.coordinates.width * scale_x,
                height=region.coordinates.height * scale_y
            )
            
            # Create new region with scaled coordinates
            scaled_region = Region(
                coordinates=scaled_bbox,
                classification=region.classification,
                confidence=region.confidence,
                content=region.content,
                metadata=region.metadata.copy() if region.metadata else {}
            )
            
            # Add scale info to metadata for debugging
            scaled_region.metadata['coordinate_scaling'] = {
                'scale_x': scale_x,
                'scale_y': scale_y,
                'original_image_width': scale_info.get('original_width'),
                'original_image_height': scale_info.get('original_height')
            }
            
            scaled_regions.append(scaled_region)
        
        return scaled_regions
    
    def _enhance_layout_classification(self, regions: List[Region], image_path: str) -> List[Region]:
        """
        Enhance layout classification with advanced heuristics
        
        Args:
            regions: Initial regions from structure analysis
            image_path: Path to preprocessed image
            
        Returns:
            Enhanced regions with better classification
        """
        try:
            # Load image for additional analysis
            image = cv2.imread(image_path)
            if image is None:
                return regions
            
            image_height, image_width = image.shape[:2]
            
            enhanced_regions = []
            
            for region in regions:
                # Create enhanced region copy
                enhanced_region = Region(
                    coordinates=region.coordinates,
                    classification=region.classification,
                    confidence=region.confidence,
                    content=region.content,
                    metadata=region.metadata.copy()
                )
                
                # Add position-based metadata
                enhanced_region.metadata.update({
                    'relative_position': {
                        'x_ratio': region.coordinates.x / image_width,
                        'y_ratio': region.coordinates.y / image_height,
                        'width_ratio': region.coordinates.width / image_width,
                        'height_ratio': region.coordinates.height / image_height
                    },
                    'area': region.coordinates.width * region.coordinates.height
                })
                
                # Refine classification based on enhanced analysis
                enhanced_region.classification = self._refine_region_classification(
                    enhanced_region, image_height, image_width
                )
                
                enhanced_regions.append(enhanced_region)
            
            return enhanced_regions
            
        except Exception as e:
            logger.warning(f"Layout enhancement failed: {e}")
            return regions
    
    def _refine_region_classification(self, region: Region, image_height: int, image_width: int) -> RegionType:
        """
        Refine region classification using advanced heuristics
        
        æ³¨æ„ï¼šæ­¤å‡½æ•°ä»…åœ¨ PaddleOCR 2.x æ—¶ä½¿ç”¨
        PaddleOCR 3.x (PPStructureV3) å·²ç»å†…ç½®äº†æ·±åº¦å­¦ä¹ å¸ƒå±€åˆ†æï¼Œä¸éœ€è¦æ­¤å‡½æ•°
        
        Args:
            region: Region to classify
            image_height: Total image height
            image_width: Total image width
            
        Returns:
            Refined RegionType
        """
        if not region.content:
            return RegionType.IMAGE
        
        text = region.content.strip()
        bbox = region.coordinates
        
        # Position-based classification
        y_ratio = bbox.y / image_height
        width_ratio = bbox.width / image_width
        height_ratio = bbox.height / image_height
        
        # List detection (enhanced) - check this first before header detection
        list_indicators = ['â€¢', '-', '*', 'â—‹', 'â–ª', 'â–«']
        numbered_pattern = any(text.startswith(f'{i}.') for i in range(1, 20))
        
        if (any(text.startswith(indicator) for indicator in list_indicators) or
            numbered_pattern or
            any(f'\n{indicator}' in text for indicator in list_indicators) or
            any(f'\n{i}.' in text for i in range(1, 20))):
            return RegionType.LIST
        
        # Header detection (enhanced)
        if (y_ratio < 0.2 or  # Top 20% of image
            (len(text) < 80 and 
             (text.isupper() or 
              any(keyword in text.lower() for keyword in ['title', 'chapter', 'section']) or
              width_ratio > 0.6))):  # Wide text likely to be header
            return RegionType.HEADER
        
        # Table detection (basic - will be enhanced in subtask 3.4)
        if (('\t' in text or '|' in text or 
             text.count(' ') > len(text) * 0.3) and  # Lots of spaces
            height_ratio > 0.1):  # Reasonable height
            return RegionType.TABLE
        
        # Default to paragraph
        return RegionType.PARAGRAPH
    
    def _sort_regions_by_reading_order(self, regions: List[Region]) -> List[Region]:
        """
        Sort regions by natural reading order (top to bottom, left to right)
        
        Args:
            regions: List of regions to sort
            
        Returns:
            Sorted list of regions
        """
        def reading_order_key(region: Region) -> Tuple[int, int]:
            # Group by approximate rows (with tolerance for slight misalignment)
            row_group = int(region.coordinates.y // 50)  # 50px tolerance
            return (row_group, int(region.coordinates.x))
        
        return sorted(regions, key=reading_order_key)
    
    def _calculate_confidence_metrics(self, regions: List[Region]) -> Dict[str, float]:
        """
        Calculate detailed confidence metrics for layout analysis
        
        ã€æ”¹è¿›ã€‘æ·»åŠ æ›´è¯¦ç»†çš„ç½®ä¿¡åº¦ç»Ÿè®¡ä¿¡æ¯ï¼ŒåŒ…æ‹¬ï¼š
        - æœ‰ç½®ä¿¡åº¦çš„åŒºåŸŸæ•°é‡
        - æ— ç½®ä¿¡åº¦çš„åŒºåŸŸæ•°é‡
        - ç½®ä¿¡åº¦è¦†ç›–ç‡
        
        Args:
            regions: List of analyzed regions
            
        Returns:
            Dictionary containing confidence metrics
        """
        if not regions:
            return {
                'overall': 0.0,
                'text_confidence': 0.0,
                'layout_confidence': 0.0,
                'region_count': 0,
                'regions_with_confidence': 0,
                'regions_without_confidence': 0,
                'confidence_coverage': 0.0
            }
        
        # åˆ†åˆ«ç»Ÿè®¡æœ‰ç½®ä¿¡åº¦å’Œæ— ç½®ä¿¡åº¦çš„åŒºåŸŸ
        regions_with_conf = [r for r in regions if r.confidence is not None and r.confidence > 0]
        regions_without_conf = [r for r in regions if r.confidence is None or r.confidence <= 0]
        
        # Calculate text confidence (average of all text confidences)
        text_confidences = [r.confidence for r in regions_with_conf]
        text_confidence = sum(text_confidences) / len(text_confidences) if text_confidences else 0.0
        
        # Calculate layout confidence based on region distribution and classification
        layout_confidence = self._calculate_layout_confidence(regions)
        
        # è®¡ç®—ç½®ä¿¡åº¦è¦†ç›–ç‡
        confidence_coverage = len(regions_with_conf) / len(regions) if regions else 0.0
        
        # Overall confidence is weighted average
        # ã€æ”¹è¿›ã€‘è€ƒè™‘ç½®ä¿¡åº¦è¦†ç›–ç‡å¯¹æ•´ä½“ç½®ä¿¡åº¦çš„å½±å“
        # å¦‚æœè¦†ç›–ç‡ä½ï¼Œæ•´ä½“ç½®ä¿¡åº¦ä¹Ÿåº”è¯¥é™ä½
        base_confidence = (text_confidence * 0.7 + layout_confidence * 0.3)
        # è¦†ç›–ç‡æƒ©ç½šï¼šè¦†ç›–ç‡ä½äº 50% æ—¶å¼€å§‹æƒ©ç½š
        coverage_penalty = min(1.0, confidence_coverage / 0.5) if confidence_coverage < 0.5 else 1.0
        overall_confidence = base_confidence * (0.5 + 0.5 * coverage_penalty)
        
        # è®°å½•æ—¥å¿—
        logger.info(f"Confidence metrics: {len(regions_with_conf)}/{len(regions)} regions have confidence "
                   f"(coverage: {confidence_coverage:.1%}), avg: {text_confidence:.4f}")
        
        # ä¿ç•™å®Œæ•´ç²¾åº¦ï¼Œä¸åšroundå¤„ç†
        return {
            'overall': overall_confidence,
            'text_confidence': text_confidence,
            'layout_confidence': layout_confidence,
            'region_count': len(regions),
            'regions_with_confidence': len(regions_with_conf),
            'regions_without_confidence': len(regions_without_conf),
            'confidence_coverage': confidence_coverage
        }
    
    def _calculate_layout_confidence(self, regions: List[Region]) -> float:
        """
        Calculate confidence score for layout analysis quality
        
        ã€ä¿®å¤ã€‘è°ƒæ•´ç½®ä¿¡åº¦è®¡ç®—é€»è¾‘ï¼š
        - é™ä½ç±»å‹å¤šæ ·æ€§çš„æƒé‡ï¼ˆæ–‡æ¡£å¯èƒ½åªæœ‰è¡¨æ ¼ä¹Ÿæ˜¯æ­£å¸¸çš„ï¼‰
        - å¢åŠ å†…å®¹è´¨é‡çš„æƒé‡
        - ä½¿ç”¨åŠ æƒå¹³å‡è€Œéç®€å•å¹³å‡
        
        Args:
            regions: List of analyzed regions
            
        Returns:
            Layout confidence score (0.0 to 1.0)
        """
        if not regions:
            return 0.0
        
        # Factor 1: Region diversity (é™ä½æƒé‡ï¼Œå› ä¸ºæ–‡æ¡£å¯èƒ½åªæœ‰ç‰¹å®šç±»å‹)
        # åªè¦æœ‰ 1 ç§ä»¥ä¸Šç±»å‹å°±ç»™è¾ƒé«˜åˆ†æ•°
        region_types = set(r.classification for r in regions)
        num_types = len(region_types)
        if num_types >= 3:
            type_diversity = 1.0
        elif num_types == 2:
            type_diversity = 0.9
        else:
            type_diversity = 0.7  # å³ä½¿åªæœ‰ä¸€ç§ç±»å‹ä¹Ÿç»™ 0.7
        
        # Factor 2: Reasonable region sizes (not too small or too large)
        reasonable_sizes = 0
        for region in regions:
            area = region.coordinates.width * region.coordinates.height
            # æ”¾å®½é¢ç§¯èŒƒå›´ï¼ŒPPStructureV3 çš„åŒºåŸŸé€šå¸¸è¾ƒå¤§
            if 50 < area < 10000000:  # æ›´å®½æ¾çš„é¢ç§¯èŒƒå›´
                reasonable_sizes += 1
        size_factor = reasonable_sizes / len(regions) if regions else 0.5
        
        # Factor 3: Text content quality (regions should have meaningful content)
        meaningful_content = 0
        for region in regions:
            if region.content and len(region.content.strip()) > 3:
                meaningful_content += 1
        content_factor = meaningful_content / len(regions) if regions else 0.5
        
        # åŠ æƒå¹³å‡ï¼šå†…å®¹è´¨é‡æƒé‡æœ€é«˜ï¼Œç±»å‹å¤šæ ·æ€§æƒé‡æœ€ä½
        # æƒé‡ï¼šå†…å®¹è´¨é‡ 0.5, å°ºå¯¸åˆç†æ€§ 0.3, ç±»å‹å¤šæ ·æ€§ 0.2
        layout_confidence = (
            content_factor * 0.5 +
            size_factor * 0.3 +
            type_diversity * 0.2
        )
        
        return layout_confidence
    
    def generate_confidence_log(self, regions: List[Region], job_id: str, output_folder: str, 
                                start_time: float = None, end_time: float = None, processing_time: float = None) -> str:
        """
        ç”Ÿæˆè¯¦ç»†çš„ç½®ä¿¡åº¦è®¡ç®—æ—¥å¿—ï¼ˆMarkdown æ ¼å¼ï¼‰
        
        æ­¤æ–¹æ³•ç”Ÿæˆä¸€ä¸ªè¯¦ç»†çš„ MD æ–‡ä»¶ï¼Œå±•ç¤ºç½®ä¿¡åº¦è®¡ç®—çš„å®Œæ•´è¿‡ç¨‹ï¼ŒåŒ…æ‹¬ï¼š
        1. å¤„ç†æ—¶é—´ä¿¡æ¯
        2. æ¯ä¸ªåŒºåŸŸçš„ç½®ä¿¡åº¦è¯¦æƒ…
        3. æ–‡æœ¬ç½®ä¿¡åº¦è®¡ç®—è¿‡ç¨‹
        4. å¸ƒå±€ç½®ä¿¡åº¦è®¡ç®—è¿‡ç¨‹
        5. æ€»ä½“ç½®ä¿¡åº¦è®¡ç®—è¿‡ç¨‹
        
        Args:
            regions: è¯†åˆ«çš„åŒºåŸŸåˆ—è¡¨
            job_id: ä»»åŠ¡ ID
            output_folder: è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„
            start_time: OCR å¤„ç†å¼€å§‹æ—¶é—´æˆ³
            end_time: OCR å¤„ç†ç»“æŸæ—¶é—´æˆ³
            processing_time: å¤„ç†è€—æ—¶ï¼ˆç§’ï¼‰
            
        Returns:
            ç”Ÿæˆçš„æ—¥å¿—æ–‡ä»¶è·¯å¾„
        """
        from datetime import datetime
        
        lines = []
        lines.append("# ç½®ä¿¡åº¦è®¡ç®—è¯¦ç»†æ—¥å¿—")
        lines.append("")
        lines.append(f"**ä»»åŠ¡ ID**: `{job_id}`")
        lines.append(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        lines.append("")
        
        # ========== å¤„ç†æ—¶é—´ä¿¡æ¯ ==========
        lines.append("---")
        lines.append("## å¤„ç†æ—¶é—´ä¿¡æ¯")
        lines.append("")
        if start_time:
            lines.append(f"- **å¼€å§‹æ—¶é—´**: {datetime.fromtimestamp(start_time).strftime('%Y-%m-%d %H:%M:%S')}")
        if end_time:
            lines.append(f"- **ç»“æŸæ—¶é—´**: {datetime.fromtimestamp(end_time).strftime('%Y-%m-%d %H:%M:%S')}")
        if processing_time is not None:
            lines.append(f"- **å¤„ç†è€—æ—¶**: {processing_time:.2f}s")
        lines.append("")
        
        # ========== 1. åŒºåŸŸæ¦‚è§ˆ ==========
        lines.append("---")
        lines.append("## 1. åŒºåŸŸæ¦‚è§ˆ")
        lines.append("")
        lines.append(f"- **æ€»åŒºåŸŸæ•°**: {len(regions)}")
        
        # åˆ†ç±»ç»Ÿè®¡
        regions_with_conf = [r for r in regions if r.confidence is not None and r.confidence > 0]
        regions_without_conf = [r for r in regions if r.confidence is None or r.confidence <= 0]
        
        lines.append(f"- **æœ‰ç½®ä¿¡åº¦çš„åŒºåŸŸ**: {len(regions_with_conf)}")
        lines.append(f"- **æ— ç½®ä¿¡åº¦çš„åŒºåŸŸ**: {len(regions_without_conf)}")
        lines.append(f"- **ç½®ä¿¡åº¦è¦†ç›–ç‡**: {len(regions_with_conf) / len(regions) * 100:.1f}%" if regions else "- **ç½®ä¿¡åº¦è¦†ç›–ç‡**: 0%")
        lines.append("")
        
        # ========== 2. æ¯ä¸ªåŒºåŸŸçš„è¯¦ç»†ä¿¡æ¯ ==========
        lines.append("---")
        lines.append("## 2. å„åŒºåŸŸç½®ä¿¡åº¦è¯¦æƒ…")
        lines.append("")
        
        if not regions:
            lines.append("*æ— è¯†åˆ«åŒºåŸŸ*")
        else:
            lines.append("| åºå· | ç±»å‹ | ä½ç½® (x,y) | å°ºå¯¸ (wÃ—h) | ç½®ä¿¡åº¦ | å†…å®¹é¢„è§ˆ |")
            lines.append("|------|------|------------|------------|--------|----------|")
            
            for i, region in enumerate(regions):
                conf_str = f"{region.confidence}" if region.confidence is not None else "æ— "
                content_preview = (region.content[:30] + "...") if region.content and len(region.content) > 30 else (region.content or "")
                content_preview = content_preview.replace("|", "\\|").replace("\n", " ")
                
                lines.append(f"| {i+1} | {region.classification.value} | ({region.coordinates.x:.0f}, {region.coordinates.y:.0f}) | {region.coordinates.width:.0f}Ã—{region.coordinates.height:.0f} | {conf_str} | {content_preview} |")
        
        lines.append("")
        
        # ========== 3. æ–‡æœ¬ç½®ä¿¡åº¦è®¡ç®— ==========
        lines.append("---")
        lines.append("## 3. æ–‡æœ¬ç½®ä¿¡åº¦è®¡ç®—")
        lines.append("")
        
        text_confidences = [r.confidence for r in regions_with_conf]
        
        if text_confidences:
            lines.append("### 3.1 æœ‰æ•ˆç½®ä¿¡åº¦å€¼åˆ—è¡¨")
            lines.append("")
            lines.append("```")
            for i, conf in enumerate(text_confidences):
                lines.append(f"  åŒºåŸŸ {i+1}: {conf}")
            lines.append("```")
            lines.append("")
            
            text_confidence = sum(text_confidences) / len(text_confidences)
            lines.append("### 3.2 è®¡ç®—è¿‡ç¨‹")
            lines.append("")
            lines.append("```")
            lines.append(f"æ–‡æœ¬ç½®ä¿¡åº¦ = æ‰€æœ‰æœ‰æ•ˆç½®ä¿¡åº¦çš„å¹³å‡å€¼")
            lines.append(f"           = ({' + '.join([f'{c}' for c in text_confidences])}) / {len(text_confidences)}")
            lines.append(f"           = {sum(text_confidences)} / {len(text_confidences)}")
            lines.append(f"           = {text_confidence}")
            lines.append("```")
        else:
            text_confidence = 0.0
            lines.append("*æ— æœ‰æ•ˆç½®ä¿¡åº¦æ•°æ®ï¼Œæ–‡æœ¬ç½®ä¿¡åº¦ = 0.0*")
        
        lines.append("")
        
        # ========== 4. å¸ƒå±€ç½®ä¿¡åº¦è®¡ç®— ==========
        lines.append("---")
        lines.append("## 4. å¸ƒå±€ç½®ä¿¡åº¦è®¡ç®—")
        lines.append("")
        
        if not regions:
            layout_confidence = 0.0
            lines.append("*æ— åŒºåŸŸæ•°æ®ï¼Œå¸ƒå±€ç½®ä¿¡åº¦ = 0.0*")
        else:
            # Factor 1: ç±»å‹å¤šæ ·æ€§
            region_types = set(r.classification for r in regions)
            num_types = len(region_types)
            if num_types >= 3:
                type_diversity = 1.0
            elif num_types == 2:
                type_diversity = 0.9
            else:
                type_diversity = 0.7
            
            lines.append("### 4.1 ç±»å‹å¤šæ ·æ€§å› å­")
            lines.append("")
            lines.append(f"- æ£€æµ‹åˆ°çš„åŒºåŸŸç±»å‹: {', '.join([t.value for t in region_types])}")
            lines.append(f"- ç±»å‹æ•°é‡: {num_types}")
            lines.append(f"- å¤šæ ·æ€§è¯„åˆ†è§„åˆ™: â‰¥3ç§ç±»å‹=1.0, 2ç§ç±»å‹=0.9, 1ç§ç±»å‹=0.7")
            lines.append(f"- **ç±»å‹å¤šæ ·æ€§å› å­**: {type_diversity:.2f}")
            lines.append("")
            
            # Factor 2: å°ºå¯¸åˆç†æ€§
            reasonable_sizes = 0
            size_details = []
            for region in regions:
                area = region.coordinates.width * region.coordinates.height
                is_reasonable = 50 < area < 10000000
                if is_reasonable:
                    reasonable_sizes += 1
                size_details.append((area, is_reasonable))
            size_factor = reasonable_sizes / len(regions)
            
            lines.append("### 4.2 å°ºå¯¸åˆç†æ€§å› å­")
            lines.append("")
            lines.append(f"- åˆç†å°ºå¯¸èŒƒå›´: 50 < é¢ç§¯ < 10,000,000 åƒç´ ")
            lines.append(f"- åˆç†å°ºå¯¸åŒºåŸŸæ•°: {reasonable_sizes} / {len(regions)}")
            lines.append(f"- **å°ºå¯¸åˆç†æ€§å› å­**: {size_factor}")
            lines.append("")
            
            # Factor 3: å†…å®¹è´¨é‡
            meaningful_content = 0
            for region in regions:
                if region.content and len(region.content.strip()) > 3:
                    meaningful_content += 1
            content_factor = meaningful_content / len(regions)
            
            lines.append("### 4.3 å†…å®¹è´¨é‡å› å­")
            lines.append("")
            lines.append(f"- æœ‰æ•ˆå†…å®¹æ ‡å‡†: å†…å®¹é•¿åº¦ > 3 å­—ç¬¦")
            lines.append(f"- æœ‰æ•ˆå†…å®¹åŒºåŸŸæ•°: {meaningful_content} / {len(regions)}")
            lines.append(f"- **å†…å®¹è´¨é‡å› å­**: {content_factor}")
            lines.append("")
            
            # è®¡ç®—å¸ƒå±€ç½®ä¿¡åº¦
            layout_confidence = content_factor * 0.5 + size_factor * 0.3 + type_diversity * 0.2
            
            lines.append("### 4.4 å¸ƒå±€ç½®ä¿¡åº¦è®¡ç®—")
            lines.append("")
            lines.append("```")
            lines.append("å¸ƒå±€ç½®ä¿¡åº¦ = å†…å®¹è´¨é‡å› å­ Ã— 0.5 + å°ºå¯¸åˆç†æ€§å› å­ Ã— 0.3 + ç±»å‹å¤šæ ·æ€§å› å­ Ã— 0.2")
            lines.append(f"           = {content_factor} Ã— 0.5 + {size_factor} Ã— 0.3 + {type_diversity} Ã— 0.2")
            lines.append(f"           = {content_factor * 0.5} + {size_factor * 0.3} + {type_diversity * 0.2}")
            lines.append(f"           = {layout_confidence}")
            lines.append("```")
        
        lines.append("")
        
        # ========== 5. æ€»ä½“ç½®ä¿¡åº¦è®¡ç®— ==========
        lines.append("---")
        lines.append("## 5. æ€»ä½“ç½®ä¿¡åº¦è®¡ç®—")
        lines.append("")
        
        if not regions:
            overall_confidence = 0.0
            lines.append("*æ— åŒºåŸŸæ•°æ®ï¼Œæ€»ä½“ç½®ä¿¡åº¦ = 0.0*")
        else:
            confidence_coverage = len(regions_with_conf) / len(regions)
            base_confidence = text_confidence * 0.7 + layout_confidence * 0.3
            coverage_penalty = min(1.0, confidence_coverage / 0.5) if confidence_coverage < 0.5 else 1.0
            overall_confidence = base_confidence * (0.5 + 0.5 * coverage_penalty)
            
            lines.append("### 5.1 åŸºç¡€ç½®ä¿¡åº¦")
            lines.append("")
            lines.append("```")
            lines.append("åŸºç¡€ç½®ä¿¡åº¦ = æ–‡æœ¬ç½®ä¿¡åº¦ Ã— 0.7 + å¸ƒå±€ç½®ä¿¡åº¦ Ã— 0.3")
            lines.append(f"           = {text_confidence} Ã— 0.7 + {layout_confidence} Ã— 0.3")
            lines.append(f"           = {text_confidence * 0.7} + {layout_confidence * 0.3}")
            lines.append(f"           = {base_confidence}")
            lines.append("```")
            lines.append("")
            
            lines.append("### 5.2 è¦†ç›–ç‡æƒ©ç½š")
            lines.append("")
            lines.append(f"- ç½®ä¿¡åº¦è¦†ç›–ç‡: {confidence_coverage} ({confidence_coverage * 100:.1f}%)")
            lines.append(f"- æƒ©ç½šè§„åˆ™: è¦†ç›–ç‡ < 50% æ—¶å¼€å§‹æƒ©ç½š")
            if confidence_coverage < 0.5:
                lines.append(f"- æƒ©ç½šå› å­ = min(1.0, {confidence_coverage} / 0.5) = {coverage_penalty}")
            else:
                lines.append(f"- è¦†ç›–ç‡ â‰¥ 50%ï¼Œæ— æƒ©ç½šï¼Œæƒ©ç½šå› å­ = 1.0")
            lines.append("")
            
            lines.append("### 5.3 æœ€ç»ˆè®¡ç®—")
            lines.append("")
            lines.append("```")
            lines.append("æ€»ä½“ç½®ä¿¡åº¦ = åŸºç¡€ç½®ä¿¡åº¦ Ã— (0.5 + 0.5 Ã— æƒ©ç½šå› å­)")
            lines.append(f"           = {base_confidence} Ã— (0.5 + 0.5 Ã— {coverage_penalty})")
            lines.append(f"           = {base_confidence} Ã— {0.5 + 0.5 * coverage_penalty}")
            lines.append(f"           = {overall_confidence}")
            lines.append("```")
        
        lines.append("")
        
        # ========== 6. ç»“æœæ±‡æ€» ==========
        lines.append("---")
        lines.append("## 6. ç»“æœæ±‡æ€»")
        lines.append("")
        lines.append("| æŒ‡æ ‡ | å€¼ |")
        lines.append("|------|-----|")
        lines.append(f"| æ€»åŒºåŸŸæ•° | {len(regions)} |")
        lines.append(f"| æœ‰ç½®ä¿¡åº¦åŒºåŸŸ | {len(regions_with_conf)} |")
        lines.append(f"| æ— ç½®ä¿¡åº¦åŒºåŸŸ | {len(regions_without_conf)} |")
        lines.append(f"| ç½®ä¿¡åº¦è¦†ç›–ç‡ | {len(regions_with_conf) / len(regions) * 100:.1f}% |" if regions else "| ç½®ä¿¡åº¦è¦†ç›–ç‡ | 0% |")
        lines.append(f"| æ–‡æœ¬ç½®ä¿¡åº¦ | {text_confidence} |")
        lines.append(f"| å¸ƒå±€ç½®ä¿¡åº¦ | {layout_confidence} |")
        lines.append(f"| **æ€»ä½“ç½®ä¿¡åº¦** | **{overall_confidence}** |")
        lines.append("")
        
        # å†™å…¥æ–‡ä»¶
        log_content = "\n".join(lines)
        log_path = os.path.join(output_folder, f"{job_id}_confidence_log.md")
        
        with open(log_path, 'w', encoding='utf-8') as f:
            f.write(log_content)
        
        logger.info(f"ç½®ä¿¡åº¦è®¡ç®—æ—¥å¿—å·²ä¿å­˜: {log_path}")
        return log_path
    
    def _parse_ppstructure_v3_to_regions(self, ppstructure_result: List[Dict[str, Any]]) -> List[Region]:
        """
        å°† PPStructureV3 çš„å¸ƒå±€åˆ†æç»“æœè½¬æ¢ä¸º Region å¯¹è±¡åˆ—è¡¨
        
        PPStructureV3 è¿”å›çš„æ¯ä¸ª item åŒ…å«ï¼š
        - type: åŒºåŸŸç±»å‹ï¼ˆtable, text, figure, figure_caption, header, footer, reference ç­‰ï¼‰
        - bbox: è¾¹ç•Œæ¡† [x1, y1, x2, y2]
        - res: å†…å®¹ï¼ˆè¡¨æ ¼ä¸º HTML å­—å…¸ï¼Œæ–‡æœ¬ä¸ºæ–‡æœ¬è¡Œåˆ—è¡¨ï¼‰
        
        ã€é‡è¦ä¿®å¤è¯´æ˜ã€‘ï¼š
        - PPStructure çš„ 'figure' ç±»å‹å¯èƒ½åŒ…å«æ–‡æœ¬å†…å®¹ï¼Œéœ€è¦æ ¹æ® res å†…å®¹åˆ¤æ–­
        - å¦‚æœ 'figure' çš„ res æ˜¯æ–‡æœ¬åˆ—è¡¨ï¼Œåº”è¯¥ä½œä¸º PARAGRAPH å¤„ç†
        - åªæœ‰å½“ res ä¸ºç©ºæˆ–ä¸åŒ…å«æ–‡æœ¬æ—¶ï¼Œæ‰ä½œä¸º IMAGE å¤„ç†
        
        Args:
            ppstructure_result: _process_ppstructure_v3_result å¤„ç†åçš„ç»“æœåˆ—è¡¨
            
        Returns:
            Region å¯¹è±¡åˆ—è¡¨
        """
        regions = []
        
        if not ppstructure_result:
            logger.warning("Empty PPStructureV3 result")
            return regions
        
        # PPStructureV3 ç±»å‹åˆ° RegionType çš„åŸºç¡€æ˜ å°„
        # æ³¨æ„ï¼šfigure ç±»å‹éœ€è¦æ ¹æ®å†…å®¹åŠ¨æ€åˆ¤æ–­
        type_mapping = {
            'table': RegionType.TABLE,
            'text': RegionType.PARAGRAPH,
            'title': RegionType.HEADER,
            'header': RegionType.HEADER,
            'footer': RegionType.PARAGRAPH,
            'figure': RegionType.IMAGE,  # é»˜è®¤å€¼ï¼Œä¼šæ ¹æ®å†…å®¹åŠ¨æ€è°ƒæ•´
            'figure_caption': RegionType.PARAGRAPH,
            'table_caption': RegionType.PARAGRAPH,
            'reference': RegionType.PARAGRAPH,
            'equation': RegionType.PARAGRAPH,
            'chart': RegionType.IMAGE,
            'seal': RegionType.IMAGE,
        }
        
        for item in ppstructure_result:
            try:
                item_type = item.get('type', 'text')
                bbox = item.get('bbox', [0, 0, 0, 0])
                res = item.get('res', [])
                # ã€æ–°å¢ã€‘è·å–åŸå§‹ PPStructureV3 ç±»å‹å’Œç¼–è¾‘ç±»å‹
                original_struct_type = item.get('original_struct_type', item_type)
                edit_type = item.get('edit_type', 'table' if item_type == 'table' else 'text')
                
                # è®¡ç®—è¾¹ç•Œæ¡†
                if len(bbox) == 4:
                    x1, y1, x2, y2 = bbox
                    bounding_box = BoundingBox(
                        x=float(x1),
                        y=float(y1),
                        width=float(x2 - x1),
                        height=float(y2 - y1)
                    )
                else:
                    continue
                
                # æå–æ–‡æœ¬å†…å®¹å’Œç½®ä¿¡åº¦
                content = ""
                confidence = None  # é»˜è®¤æ— ç½®ä¿¡åº¦
                has_text_content = False  # æ ‡è®°æ˜¯å¦æœ‰æ–‡æœ¬å†…å®¹
                
                if item_type == 'table':
                    # è¡¨æ ¼ç±»å‹ï¼šres æ˜¯åŒ…å« html çš„å­—å…¸
                    if isinstance(res, dict):
                        content = res.get('html', '')
                        # è¡¨æ ¼çš„ç½®ä¿¡åº¦ï¼šSLANet æ¨¡å‹ä¸è¾“å‡ºç½®ä¿¡åº¦ï¼Œè®¾ä¸º None
                        confidence = res.get('confidence', None)
                else:
                    # å…¶ä»–ç±»å‹ï¼šres æ˜¯æ–‡æœ¬è¡Œåˆ—è¡¨
                    if isinstance(res, list):
                        text_parts = []
                        confidences = []
                        for text_item in res:
                            if isinstance(text_item, dict):
                                text = text_item.get('text', '')
                                conf = text_item.get('confidence')  # å¯èƒ½ä¸º None
                                if text:
                                    text_parts.append(text)
                                    if conf is not None:
                                        confidences.append(conf)
                        content = '\n'.join(text_parts)
                        # åªæœ‰å½“æœ‰çœŸå®ç½®ä¿¡åº¦æ—¶æ‰è®¡ç®—å¹³å‡å€¼
                        if confidences:
                            confidence = sum(confidences) / len(confidences)
                        else:
                            confidence = None  # æ— ç½®ä¿¡åº¦
                        has_text_content = len(text_parts) > 0
                    elif isinstance(res, str):
                        content = res
                        has_text_content = bool(res.strip())
                        confidence = None  # çº¯å­—ç¬¦ä¸²æ— ç½®ä¿¡åº¦
                
                # ã€å…³é”®ä¿®å¤ã€‘åŠ¨æ€åˆ¤æ–­ figure ç±»å‹çš„å®é™…åˆ†ç±»
                # PPStructure çš„ 'figure' ç±»å‹å¯èƒ½åŒ…å«æ–‡æœ¬å†…å®¹æˆ– HTML è¡¨æ ¼
                if item_type == 'figure' and has_text_content:
                    # æ£€æŸ¥å†…å®¹æ˜¯å¦æ˜¯ HTML è¡¨æ ¼
                    content_lower = content.lower().strip()
                    if content_lower.startswith('<html') or content_lower.startswith('<table') or '<table>' in content_lower:
                        # å†…å®¹æ˜¯ HTML è¡¨æ ¼ï¼Œä½œä¸º TABLE å¤„ç†
                        region_type = RegionType.TABLE
                        logger.debug(f"Figure with HTML table content treated as TABLE")
                    else:
                        # æ™®é€šæ–‡æœ¬å†…å®¹ï¼Œä½œä¸º PARAGRAPH å¤„ç†
                        region_type = RegionType.PARAGRAPH
                        logger.debug(f"Figure with text content treated as PARAGRAPH: {content[:50]}...")
                else:
                    # ä½¿ç”¨é»˜è®¤æ˜ å°„
                    region_type = type_mapping.get(item_type, RegionType.PARAGRAPH)
                
                # è·³è¿‡ç©ºå†…å®¹çš„åŒºåŸŸï¼ˆé™¤äº†è¡¨æ ¼ï¼‰
                if not content and item_type != 'table':
                    logger.debug(f"Skipping empty {item_type} region")
                    continue
                
                # ã€æ–°å¢ã€‘æ„å»º metadataï¼ŒåŒ…å«åŸå§‹ç±»å‹ä¿¡æ¯
                region_metadata = {
                    'originalStructType': original_struct_type,  # PPStructureV3 åŸå§‹ç±»å‹
                    'editType': edit_type,  # ç¼–è¾‘ç±»å‹: text æˆ– table
                }
                
                region = Region(
                    coordinates=bounding_box,
                    classification=region_type,
                    confidence=confidence,
                    content=content,
                    metadata=region_metadata
                )
                
                regions.append(region)
                logger.debug(f"Created region: type={region_type.value}, struct_type={original_struct_type}, content_len={len(content)}")
                
            except Exception as e:
                logger.warning(f"Failed to parse PPStructureV3 item: {e}")
                continue
        
        logger.info(f"Parsed {len(regions)} regions from PPStructureV3 result")
        return regions

    def _parse_structure_result(self, structure_result: List) -> List[Region]:
        """
        Parse PaddleOCR structure result into Region objects
        
        Args:
            structure_result: Raw PaddleOCR structure result
            
        Returns:
            List of Region objects
        """
        regions = []
        
        # PaddleOCR returns results in format: [[[bbox, text_info], ...]]
        # We need to unwrap the outer list first
        if not structure_result or len(structure_result) == 0:
            logger.warning("Empty structure result from PaddleOCR")
            return regions
        
        # Get the actual results from the first element
        actual_results = structure_result[0] if structure_result else []
        
        for item in actual_results:
            if not item or len(item) < 2:
                continue
            
            try:
                # Extract bounding box coordinates
                bbox_coords = item[0]
                if len(bbox_coords) != 4 or len(bbox_coords[0]) != 2:
                    continue
                
                # Calculate bounding box
                x_coords = [point[0] for point in bbox_coords]
                y_coords = [point[1] for point in bbox_coords]
                
                bbox = BoundingBox(
                    x=min(x_coords),
                    y=min(y_coords),
                    width=max(x_coords) - min(x_coords),
                    height=max(y_coords) - min(y_coords)
                )
                
                # Extract text and confidence
                text_info = item[1]
                if isinstance(text_info, tuple) and len(text_info) >= 2:
                    text_content = text_info[0]
                    confidence = float(text_info[1])
                else:
                    text_content = str(text_info)
                    confidence = 0.8  # Default confidence
                
                # Classify region type based on content and position
                region_type = self._classify_region(text_content, bbox)
                
                region = Region(
                    coordinates=bbox,
                    classification=region_type,
                    confidence=confidence,
                    content=text_content
                )
                
                regions.append(region)
                
            except Exception as e:
                logger.warning(f"Failed to parse structure item: {e}")
                continue
        
        return regions
    
    def _classify_region(self, text_content: str, bbox: BoundingBox) -> RegionType:
        """
        Classify region type based on content and position
        
        Args:
            text_content: Extracted text content
            bbox: Bounding box of the region
            
        Returns:
            RegionType classification
        """
        if not text_content or not text_content.strip():
            return RegionType.IMAGE
        
        text = text_content.strip()
        
        # Simple heuristics for classification
        # This can be enhanced with more sophisticated ML models
        
        # Check for list patterns first (before header detection)
        list_indicators = ['â€¢', '-', '*', 'â—‹', 'â–ª', 'â–«']
        numbered_pattern = any(text.startswith(f'{i}.') for i in range(1, 20))
        
        if (any(text.startswith(indicator) for indicator in list_indicators) or
            numbered_pattern or
            any(f'\n{indicator}' in text for indicator in list_indicators) or
            any(f'\n{i}.' in text for i in range(1, 20))):
            return RegionType.LIST
        
        # Check for header patterns
        if (len(text) < 100 and 
            (text.isupper() or 
             any(char.isdigit() for char in text[:10]) or
             bbox.y < 100)):  # Likely header if near top
            return RegionType.HEADER
        
        # Default to paragraph for regular text
        return RegionType.PARAGRAPH
    
    def extract_text(self, image_path: str, regions: List[Region]) -> List[Region]:
        """
        Extract text from specified regions using OCR with retry mechanism
        
        PaddleOCR 3.x API é€‚é…ï¼š
        - ä½¿ç”¨ predict æ–¹æ³•æ›¿ä»£ ocr æ–¹æ³•
        - å¤„ç†æ–°çš„è¿”å›æ ¼å¼
        
        Args:
            image_path: Path to image file
            regions: List of regions to extract text from
            
        Returns:
            List of regions with updated text content
        """
        if not self._ocr_engine:
            raise OCRProcessingError("OCR engine not initialized")
        
        @retry_handler.retry(RetryConfig(max_retries=3, base_delay=1.0))
        def perform_text_extraction():
            try:
                import paddleocr
                version = getattr(paddleocr, '__version__', '2.0.0')
                is_v3 = version.startswith('3.')
                
                # If no specific regions provided, use full image OCR
                if not regions:
                    if is_v3:
                        result = list(self._ocr_engine.predict(image_path))
                        result = self._convert_v3_result_to_legacy(result)
                    else:
                        result = self._ocr_engine.ocr(image_path, cls=True)
                    return self._parse_ocr_result(result)
                
                # Extract text from specific regions
                updated_regions = []
                image = cv2.imread(image_path)
                
                for region in regions:
                    try:
                        # Crop region from image
                        x = int(region.coordinates.x)
                        y = int(region.coordinates.y)
                        w = int(region.coordinates.width)
                        h = int(region.coordinates.height)
                        
                        cropped = image[y:y+h, x:x+w]
                        
                        # Save cropped region temporarily
                        temp_path = f"/tmp/region_{hash(str(region.coordinates))}.jpg"
                        cv2.imwrite(temp_path, cropped)
                        
                        # Perform OCR on cropped region with retry
                        if is_v3:
                            ocr_result = list(self._ocr_engine.predict(temp_path))
                            ocr_result = self._convert_v3_result_to_legacy(ocr_result)
                        else:
                            ocr_result = self._ocr_engine.ocr(temp_path, cls=True)
                        
                        # Update region with OCR result
                        if ocr_result and ocr_result[0]:
                            text_parts = []
                            confidences = []
                            
                            for line in ocr_result[0]:
                                if len(line) >= 2:
                                    text_parts.append(line[1][0])
                                    confidences.append(line[1][1])
                            
                            region.content = ' '.join(text_parts)
                            region.confidence = sum(confidences) / len(confidences) if confidences else 0.0
                        
                        updated_regions.append(region)
                        
                        # Clean up temporary file
                        try:
                            os.remove(temp_path)
                        except OSError:
                            pass
                            
                    except Exception as e:
                        logger.warning(f"Failed to extract text from region: {e}")
                        updated_regions.append(region)
                
                return updated_regions
                
            except Exception as e:
                # Convert certain errors to retryable network errors
                if any(keyword in str(e).lower() for keyword in ['network', 'connection', 'timeout', 'model']):
                    raise NetworkRetryError(f"Text extraction network error: {e}")
                else:
                    raise OCRProcessingError(f"Text extraction failed: {e}")
        
        try:
            return perform_text_extraction()
        except NetworkRetryError as e:
            raise OCRProcessingError(f"Text extraction failed after retries: {e}")
        except Exception as e:
            raise OCRProcessingError(f"Text extraction error: {e}")
    
    def _parse_ocr_result(self, ocr_result: List) -> List[Region]:
        """
        Parse standard OCR result into Region objects
        
        Args:
            ocr_result: Raw PaddleOCR result
            
        Returns:
            List of Region objects
        """
        regions = []
        
        if not ocr_result or not ocr_result[0]:
            return regions
        
        for line in ocr_result[0]:
            if len(line) < 2:
                continue
            
            try:
                # Extract coordinates
                bbox_coords = line[0]
                x_coords = [point[0] for point in bbox_coords]
                y_coords = [point[1] for point in bbox_coords]
                
                bbox = BoundingBox(
                    x=min(x_coords),
                    y=min(y_coords),
                    width=max(x_coords) - min(x_coords),
                    height=max(y_coords) - min(y_coords)
                )
                
                # Extract text and confidence
                text_content = line[1][0]
                confidence = line[1][1]
                
                # Classify region
                region_type = self._classify_region(text_content, bbox)
                
                region = Region(
                    coordinates=bbox,
                    classification=region_type,
                    confidence=confidence,
                    content=text_content
                )
                
                regions.append(region)
                
            except Exception as e:
                logger.warning(f"Failed to parse OCR line: {e}")
                continue
        
        return regions
    
    def extract_tables(self, image_path: str, regions: List[Region]) -> List[TableStructure]:
        """
        Extract table structures from regions using PP-Structure table recognition
        
        Args:
            image_path: Path to image file
            regions: List of regions that might contain tables
            
        Returns:
            List of TableStructure objects
        """
        if not self._structure_engine:
            raise OCRProcessingError("Structure engine not initialized")
        
        try:
            tables = []
            image = cv2.imread(image_path)
            
            if image is None:
                raise OCRProcessingError(f"Could not load image: {image_path}")
            
            # Filter regions that are likely to be tables
            table_regions = [r for r in regions if r.classification == RegionType.TABLE]
            
            # If no table regions identified, try to detect tables in the full image
            if not table_regions:
                tables.extend(self._detect_tables_in_full_image(image_path))
            else:
                # Process each table region
                for region in table_regions:
                    table_structure = self._extract_table_from_region(image, region)
                    if table_structure:
                        tables.append(table_structure)
            
            logger.info(f"Extracted {len(tables)} table structures")
            return tables
            
        except Exception as e:
            raise OCRProcessingError(f"Table extraction failed: {e}")
    
    def _detect_tables_in_full_image(self, image_path: str) -> List[TableStructure]:
        """
        Detect tables in the full image using PP-Structure
        
        PaddleOCR 3.x API é€‚é…ï¼š
        - ä½¿ç”¨ PPStructureV3 æ›¿ä»£ PPStructure
        - ä½¿ç”¨ç¼“å­˜çš„å•ä¾‹å®ä¾‹ï¼Œé¿å…é‡å¤åŠ è½½æ¨¡å‹
        - ä¼˜å…ˆä½¿ç”¨ç¼“å­˜çš„ PPStructure ç»“æœï¼Œé¿å…é‡å¤å¤„ç†
        
        Args:
            image_path: Path to image file
            
        Returns:
            List of detected TableStructure objects
        """
        try:
            # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜çš„ PPStructure ç»“æœ
            # è¿™æ ·å¯ä»¥é¿å…é‡å¤è°ƒç”¨ predict()
            if image_path in self._ppstructure_result_cache:
                logger.info(f"Using cached PPStructure result for {image_path}")
                processed_result = self._ppstructure_result_cache[image_path]
            else:
                # æ²¡æœ‰ç¼“å­˜ï¼Œéœ€è¦è°ƒç”¨ PPStructure
                # ä½¿ç”¨ç¼“å­˜çš„ PPStructureV3 å®ä¾‹ï¼ˆé¿å…é‡å¤åŠ è½½æ¨¡å‹ï¼‰
                table_engine = get_ppstructure_v3_instance()
                
                if table_engine is not None:
                    logger.info("Using cached PPStructureV3 instance (PaddleOCR 3.x)")
                else:
                    # å›é€€åˆ°æ—§ç‰ˆ PPStructure (PaddleOCR 2.x)
                    try:
                        from paddleocr import PPStructure
                        table_engine = PPStructure(
                            use_gpu=self.use_gpu,
                            show_log=False,
                            lang=self.lang,
                            layout=True,
                            table=True,
                            ocr=True,
                            recovery=True,
                        )
                        logger.info("Using PPStructure (PaddleOCR 2.x fallback)")
                    except ImportError:
                        logger.warning("PPStructure not available, using fallback")
                        return self._fallback_table_detection(image_path)
                
                # Perform table detection
                # ç¦ç”¨ä¸å¿…è¦çš„åŠŸèƒ½ä»¥åŠ é€Ÿå¤„ç†
                result = table_engine.predict(
                    image_path,
                    use_doc_orientation_classify=False,
                    use_doc_unwarping=False,
                    use_seal_recognition=False,
                    use_formula_recognition=False,
                    use_chart_recognition=False
                )
                
                # PPStructureV3 è¿”å›æ ¼å¼ä¸åŒï¼Œéœ€è¦é€‚é…
                if hasattr(result, '__iter__') and not isinstance(result, (str, dict)):
                    result_list = list(result)
                else:
                    result_list = [result] if result else []
                
                logger.info(f"PPStructure returned {len(result_list)} items")
                
                # å¤„ç† PPStructureV3 çš„è¿”å›æ ¼å¼
                processed_result = self._process_ppstructure_v3_result(result_list, image_path)
                
                # ç¼“å­˜ç»“æœ
                self._ppstructure_result_cache[image_path] = processed_result
            
            # Save raw PPStructure HTML output
            self._save_ppstructure_html(image_path, processed_result)
            
            tables = []
            for idx, item in enumerate(processed_result):
                item_type = item.get('type', 'unknown')
                logger.info(f"Item {idx}: type={item_type}, keys={list(item.keys())}")
                
                if item_type == 'table':
                    table_structure = self._parse_table_result(item)
                    if table_structure:
                        if table_structure.rows > 20:
                            split_tables = self._split_large_table(table_structure)
                            tables.extend(split_tables)
                            logger.info(f"Split large table into {len(split_tables)} tables")
                        else:
                            tables.append(table_structure)
                            logger.info(f"Successfully parsed table {idx}")
                    else:
                        logger.warning(f"Failed to parse table {idx}")
            
            logger.info(f"Total tables extracted: {len(tables)}")
            return tables
            
        except ImportError as e:
            logger.warning(f"PPStructure not available: {e}, using fallback table detection")
            return self._fallback_table_detection(image_path)
        except Exception as e:
            logger.warning(f"Table detection failed: {e}")
            import traceback
            logger.warning(traceback.format_exc())
            return []
    
    def _process_ppstructure_v3_result(self, result_list: List, image_path: str) -> List[Dict[str, Any]]:
        """
        å¤„ç† PPStructureV3 çš„è¿”å›ç»“æœï¼Œè½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
        
        PPStructureV3 è¿”å› LayoutParsingResultV2 å¯¹è±¡ï¼ˆdict-likeï¼‰ï¼ŒåŒ…å«ï¼š
        - parsing_res_list: LayoutBlock å¯¹è±¡åˆ—è¡¨ï¼Œæ¯ä¸ªå¯¹è±¡æœ‰ block_label, block_bbox, block_content å±æ€§
        - layout_det_res: å¸ƒå±€æ£€æµ‹ç»“æœ
        - table_res_list: è¡¨æ ¼è¯†åˆ«ç»“æœï¼ˆåŒ…å« OCR ç½®ä¿¡åº¦ï¼‰
        - overall_ocr_res: æ•´ä½“ OCR ç»“æœï¼ˆåŒ…å«æ‰€æœ‰æ–‡æœ¬è¡Œçš„ç½®ä¿¡åº¦ï¼‰
        
        ã€é‡è¦ã€‘PPStructureV3 ç»“æœå¯¹è±¡æ˜¯ dict-likeï¼Œå¿…é¡»ä½¿ç”¨ result['key'] è®¿é—®ï¼Œ
        è€Œä¸æ˜¯ getattr(result, 'key')
        
        ç½®ä¿¡åº¦è·å–ç­–ç•¥ï¼ˆPaddleOCR 3.x æ”¹è¿›ç‰ˆï¼‰ï¼š
        - è¡¨æ ¼åŒºå—ï¼šä» table_res_list[x].table_ocr_pred.rec_scores è·å–å¹³å‡ç½®ä¿¡åº¦
        - éè¡¨æ ¼åŒºå—ï¼šä» overall_ocr_res è·å–æ–‡æœ¬è¡Œç½®ä¿¡åº¦ï¼Œé€šè¿‡ä½ç½®åŒ¹é…å…³è”åˆ°åŒºå—
        
        Args:
            result_list: PPStructureV3 è¿”å›çš„ç»“æœåˆ—è¡¨ï¼ˆé€šå¸¸åªæœ‰ä¸€ä¸ªé¡µé¢ç»“æœï¼‰
            image_path: å›¾åƒè·¯å¾„
            
        Returns:
            ç»Ÿä¸€æ ¼å¼çš„ç»“æœåˆ—è¡¨ï¼Œå…¼å®¹æ—§ç‰ˆ PPStructure æ ¼å¼
        """
        processed = []
        
        for result in result_list:
            # PPStructureV3 è¿”å› LayoutParsingResultV2 å¯¹è±¡ï¼ˆdict-likeï¼‰
            # ã€é‡è¦ã€‘å¿…é¡»ä½¿ç”¨ [] è®¿é—®ï¼Œä¸èƒ½ç”¨ hasattr/getattr
            parsing_res_list = None
            table_res_list = None
            overall_ocr_res = None
            
            # å°è¯• dict-like è®¿é—®ï¼ˆPPStructureV3 çš„æ­£ç¡®æ–¹å¼ï¼‰
            try:
                if hasattr(result, '__getitem__') and hasattr(result, 'keys'):
                    # dict-like å¯¹è±¡ï¼Œä½¿ç”¨ [] è®¿é—®
                    parsing_res_list = result.get('parsing_res_list') if hasattr(result, 'get') else result['parsing_res_list']
                    table_res_list = result.get('table_res_list', []) if hasattr(result, 'get') else result.get('table_res_list', [])
                    overall_ocr_res = result.get('overall_ocr_res') if hasattr(result, 'get') else result.get('overall_ocr_res')
                    logger.debug(f"PPStructureV3 result keys: {list(result.keys())}")
                elif isinstance(result, dict):
                    parsing_res_list = result.get('parsing_res_list')
                    table_res_list = result.get('table_res_list', [])
                    overall_ocr_res = result.get('overall_ocr_res')
                else:
                    # å›é€€åˆ°å±æ€§è®¿é—®
                    parsing_res_list = getattr(result, 'parsing_res_list', None)
                    table_res_list = getattr(result, 'table_res_list', [])
                    overall_ocr_res = getattr(result, 'overall_ocr_res', None)
            except (KeyError, TypeError) as e:
                logger.warning(f"Failed to access PPStructureV3 result: {e}")
                # å›é€€åˆ°å±æ€§è®¿é—®
                parsing_res_list = getattr(result, 'parsing_res_list', None)
                table_res_list = getattr(result, 'table_res_list', [])
                overall_ocr_res = getattr(result, 'overall_ocr_res', None)
            
            # ã€æ–°å¢ã€‘ä» overall_ocr_res æå–æ–‡æœ¬è¡Œç½®ä¿¡åº¦å’Œä½ç½®ä¿¡æ¯
            # overall_ocr_res åŒ…å« dt_polys (æ£€æµ‹æ¡†), rec_texts (è¯†åˆ«æ–‡æœ¬), rec_scores (è¯†åˆ«ç½®ä¿¡åº¦)
            ocr_text_lines = self._extract_ocr_text_lines_with_confidence(overall_ocr_res)
            if ocr_text_lines:
                logger.info(f"Extracted {len(ocr_text_lines)} text lines with confidence from overall_ocr_res")
            
            # æ„å»ºè¡¨æ ¼åŒºåŸŸåˆ°ç½®ä¿¡åº¦çš„æ˜ å°„
            # ä» table_res_list[x].table_ocr_pred.rec_scores è·å–
            table_confidence_map = {}
            if table_res_list:
                for table_idx, table_res in enumerate(table_res_list):
                    try:
                        # table_res ä¹Ÿæ˜¯ dict-like å¯¹è±¡
                        table_ocr_pred = None
                        if hasattr(table_res, '__getitem__'):
                            table_ocr_pred = table_res.get('table_ocr_pred') if hasattr(table_res, 'get') else None
                        if table_ocr_pred is None:
                            table_ocr_pred = getattr(table_res, 'table_ocr_pred', None)
                        
                        if table_ocr_pred:
                            # table_ocr_pred ä¹Ÿæ˜¯ dict-likeï¼Œrec_scores æ˜¯ numpy array
                            rec_scores = None
                            if hasattr(table_ocr_pred, '__getitem__'):
                                rec_scores = table_ocr_pred.get('rec_scores') if hasattr(table_ocr_pred, 'get') else None
                            if rec_scores is None:
                                rec_scores = getattr(table_ocr_pred, 'rec_scores', None)
                            
                            if rec_scores is not None and len(rec_scores) > 0:
                                # rec_scores å¯èƒ½æ˜¯ numpy arrayï¼Œéœ€è¦è½¬æ¢
                                if hasattr(rec_scores, 'tolist'):
                                    rec_scores = rec_scores.tolist()
                                avg_confidence = sum(rec_scores) / len(rec_scores)
                                table_confidence_map[table_idx] = avg_confidence
                                logger.info(f"Table {table_idx} average OCR confidence: {avg_confidence:.4f} (from {len(rec_scores)} cells)")
                    except Exception as e:
                        logger.warning(f"Failed to extract confidence for table {table_idx}: {e}")
            
            if parsing_res_list:
                logger.info(f"Processing {len(parsing_res_list)} layout blocks from parsing_res_list")
                # å¤„ç† LayoutBlock å¯¹è±¡åˆ—è¡¨
                table_block_idx = 0  # ç”¨äºåŒ¹é… table_res_list
                for block_idx, block in enumerate(parsing_res_list):
                    # ã€é‡è¦ã€‘block å¯¹è±¡ä½¿ç”¨ block_label, block_content, block_bbox å±æ€§
                    # å¯èƒ½æ˜¯ dict-like æˆ–æ™®é€šå¯¹è±¡
                    label = None
                    block_bbox = None
                    try:
                        if hasattr(block, '__getitem__'):
                            label = block.get('block_label') if hasattr(block, 'get') else block['block_label']
                            block_bbox = block.get('block_bbox') if hasattr(block, 'get') else block.get('block_bbox')
                        if label is None:
                            label = getattr(block, 'block_label', None) or getattr(block, 'label', None)
                        if block_bbox is None:
                            block_bbox = getattr(block, 'block_bbox', None) or getattr(block, 'bbox', None)
                    except (KeyError, TypeError):
                        label = getattr(block, 'block_label', None) or getattr(block, 'label', None)
                        block_bbox = getattr(block, 'block_bbox', None) or getattr(block, 'bbox', None)
                    
                    # è·å–ç½®ä¿¡åº¦
                    block_confidence = None
                    if label == 'table' and table_block_idx in table_confidence_map:
                        block_confidence = table_confidence_map[table_block_idx]
                        table_block_idx += 1
                    elif label == 'table':
                        table_block_idx += 1
                    else:
                        # ã€æ–°å¢ã€‘éè¡¨æ ¼åŒºå—ï¼šä» overall_ocr_res åŒ¹é…ç½®ä¿¡åº¦
                        if ocr_text_lines and block_bbox is not None:
                            block_confidence = self._match_block_confidence_from_ocr(block_bbox, ocr_text_lines)
                    
                    item_dict = self._convert_layout_block_to_dict(block, block_confidence)
                    if item_dict:
                        processed.append(item_dict)
                        logger.debug(f"Block {block_idx}: type={item_dict.get('type')}, confidence={block_confidence}")
            else:
                logger.warning("parsing_res_list is None or empty, trying fallback processing")
                # å›é€€ï¼šå°è¯•æ—§æ ¼å¼å¤„ç†
                if isinstance(result, dict):
                    if 'type' in result:
                        processed.append(result)
                    else:
                        for key, value in result.items():
                            if isinstance(value, list):
                                for item in value:
                                    if isinstance(item, dict) and 'type' in item:
                                        processed.append(item)
                elif hasattr(result, '__dict__'):
                    item_dict = {}
                    if hasattr(result, 'type'):
                        item_dict['type'] = result.type
                    if hasattr(result, 'bbox'):
                        item_dict['bbox'] = result.bbox
                    if hasattr(result, 'res'):
                        item_dict['res'] = result.res
                    if hasattr(result, 'html'):
                        item_dict['res'] = {'html': result.html}
                    if item_dict:
                        processed.append(item_dict)
        
        if not processed and result_list:
            logger.warning(f"Could not process PPStructureV3 result, result_list has {len(result_list)} items")
        else:
            logger.info(f"Processed {len(processed)} items from PPStructureV3 result")
        
        return processed
    
    def _extract_ocr_text_lines_with_confidence(self, overall_ocr_res) -> List[Dict[str, Any]]:
        """
        ä» overall_ocr_res æå–æ‰€æœ‰æ–‡æœ¬è¡Œçš„ç½®ä¿¡åº¦å’Œä½ç½®ä¿¡æ¯
        
        overall_ocr_res æ˜¯ PPStructureV3 çš„æ•´ä½“ OCR ç»“æœï¼ŒåŒ…å«ï¼š
        - dt_polys: æ£€æµ‹æ¡†åæ ‡ (N, 4, 2) - Nä¸ªæ£€æµ‹æ¡†ï¼Œæ¯ä¸ªæ¡†4ä¸ªç‚¹
        - rec_texts: è¯†åˆ«çš„æ–‡æœ¬åˆ—è¡¨
        - rec_scores: è¯†åˆ«ç½®ä¿¡åº¦åˆ—è¡¨
        
        Args:
            overall_ocr_res: PPStructureV3 çš„ overall_ocr_res å­—æ®µ
            
        Returns:
            æ–‡æœ¬è¡Œåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« bbox, text, confidence
        """
        text_lines = []
        
        if overall_ocr_res is None:
            return text_lines
        
        try:
            # è·å–æ£€æµ‹æ¡†ã€æ–‡æœ¬å’Œç½®ä¿¡åº¦
            dt_polys = None
            rec_texts = None
            rec_scores = None
            
            # å°è¯• dict-like è®¿é—®
            if hasattr(overall_ocr_res, '__getitem__'):
                try:
                    dt_polys = overall_ocr_res.get('dt_polys') if hasattr(overall_ocr_res, 'get') else overall_ocr_res['dt_polys']
                    rec_texts = overall_ocr_res.get('rec_texts') if hasattr(overall_ocr_res, 'get') else overall_ocr_res['rec_texts']
                    rec_scores = overall_ocr_res.get('rec_scores') if hasattr(overall_ocr_res, 'get') else overall_ocr_res['rec_scores']
                except (KeyError, TypeError):
                    pass
            
            # å›é€€åˆ°å±æ€§è®¿é—®
            if dt_polys is None:
                dt_polys = getattr(overall_ocr_res, 'dt_polys', None)
            if rec_texts is None:
                rec_texts = getattr(overall_ocr_res, 'rec_texts', None)
            if rec_scores is None:
                rec_scores = getattr(overall_ocr_res, 'rec_scores', None)
            
            if dt_polys is None or rec_scores is None:
                logger.debug("overall_ocr_res missing dt_polys or rec_scores")
                return text_lines
            
            # è½¬æ¢ numpy array ä¸ºåˆ—è¡¨
            if hasattr(dt_polys, 'tolist'):
                dt_polys = dt_polys.tolist()
            if hasattr(rec_scores, 'tolist'):
                rec_scores = rec_scores.tolist()
            if rec_texts is not None and hasattr(rec_texts, 'tolist'):
                rec_texts = rec_texts.tolist()
            
            # æ„å»ºæ–‡æœ¬è¡Œåˆ—è¡¨
            for i, poly in enumerate(dt_polys):
                if i >= len(rec_scores):
                    break
                
                # è®¡ç®—è¾¹ç•Œæ¡† [x1, y1, x2, y2]
                if len(poly) >= 4:
                    x_coords = [p[0] for p in poly]
                    y_coords = [p[1] for p in poly]
                    bbox = [min(x_coords), min(y_coords), max(x_coords), max(y_coords)]
                else:
                    continue
                
                text = rec_texts[i] if rec_texts and i < len(rec_texts) else ''
                confidence = float(rec_scores[i])
                
                text_lines.append({
                    'bbox': bbox,
                    'text': text,
                    'confidence': confidence,
                    'poly': poly
                })
            
            logger.debug(f"Extracted {len(text_lines)} text lines from overall_ocr_res")
            
        except Exception as e:
            logger.warning(f"Failed to extract text lines from overall_ocr_res: {e}")
            import traceback
            logger.debug(traceback.format_exc())
        
        return text_lines
    
    def _match_block_confidence_from_ocr(self, block_bbox, ocr_text_lines: List[Dict[str, Any]]) -> Optional[float]:
        """
        æ ¹æ®å¸ƒå±€åŒºå—çš„ä½ç½®ï¼Œä» OCR æ–‡æœ¬è¡Œä¸­åŒ¹é…å¹¶è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
        
        åŒ¹é…ç­–ç•¥ï¼š
        1. è®¡ç®—æ¯ä¸ª OCR æ–‡æœ¬è¡Œä¸å¸ƒå±€åŒºå—çš„ IoU (Intersection over Union)
        2. å¦‚æœ IoU > 0.3 æˆ–æ–‡æœ¬è¡Œä¸­å¿ƒç‚¹åœ¨åŒºå—å†…ï¼Œåˆ™è®¤ä¸ºè¯¥æ–‡æœ¬è¡Œå±äºè¯¥åŒºå—
        3. è®¡ç®—æ‰€æœ‰åŒ¹é…æ–‡æœ¬è¡Œçš„å¹³å‡ç½®ä¿¡åº¦
        
        Args:
            block_bbox: å¸ƒå±€åŒºå—çš„è¾¹ç•Œæ¡† [x1, y1, x2, y2] æˆ– numpy array
            ocr_text_lines: OCR æ–‡æœ¬è¡Œåˆ—è¡¨
            
        Returns:
            å¹³å‡ç½®ä¿¡åº¦ï¼Œå¦‚æœæ²¡æœ‰åŒ¹é…çš„æ–‡æœ¬è¡Œåˆ™è¿”å› None
        """
        if not ocr_text_lines or block_bbox is None:
            return None
        
        try:
            # è½¬æ¢ block_bbox ä¸ºåˆ—è¡¨
            if hasattr(block_bbox, 'tolist'):
                block_bbox = block_bbox.tolist()
            block_bbox = list(block_bbox)
            
            if len(block_bbox) != 4:
                return None
            
            bx1, by1, bx2, by2 = block_bbox
            block_area = (bx2 - bx1) * (by2 - by1)
            
            if block_area <= 0:
                return None
            
            matched_confidences = []
            
            for text_line in ocr_text_lines:
                line_bbox = text_line.get('bbox', [])
                if len(line_bbox) != 4:
                    continue
                
                lx1, ly1, lx2, ly2 = line_bbox
                
                # è®¡ç®—æ–‡æœ¬è¡Œä¸­å¿ƒç‚¹
                center_x = (lx1 + lx2) / 2
                center_y = (ly1 + ly2) / 2
                
                # æ£€æŸ¥ä¸­å¿ƒç‚¹æ˜¯å¦åœ¨åŒºå—å†…
                center_in_block = (bx1 <= center_x <= bx2) and (by1 <= center_y <= by2)
                
                # è®¡ç®— IoU
                inter_x1 = max(bx1, lx1)
                inter_y1 = max(by1, ly1)
                inter_x2 = min(bx2, lx2)
                inter_y2 = min(by2, ly2)
                
                if inter_x2 > inter_x1 and inter_y2 > inter_y1:
                    inter_area = (inter_x2 - inter_x1) * (inter_y2 - inter_y1)
                    line_area = (lx2 - lx1) * (ly2 - ly1)
                    union_area = block_area + line_area - inter_area
                    iou = inter_area / union_area if union_area > 0 else 0
                else:
                    iou = 0
                
                # å¦‚æœ IoU > 0.3 æˆ–ä¸­å¿ƒç‚¹åœ¨åŒºå—å†…ï¼Œåˆ™åŒ¹é…
                if iou > 0.3 or center_in_block:
                    confidence = text_line.get('confidence')
                    if confidence is not None:
                        matched_confidences.append(confidence)
            
            if matched_confidences:
                avg_confidence = sum(matched_confidences) / len(matched_confidences)
                logger.debug(f"Block matched {len(matched_confidences)} text lines, avg confidence: {avg_confidence:.4f}")
                return avg_confidence
            
        except Exception as e:
            logger.warning(f"Failed to match block confidence: {e}")
        
        return None
    
    def _convert_layout_block_to_dict(self, block, block_confidence: Optional[float] = None) -> Optional[Dict[str, Any]]:
        """
        å°† PPStructureV3 çš„ LayoutBlock å¯¹è±¡è½¬æ¢ä¸ºç»Ÿä¸€çš„å­—å…¸æ ¼å¼
        
        ã€é‡è¦ã€‘PPStructureV3 çš„ LayoutBlock å±æ€§åç§°ï¼š
        - block_label: åŒºåŸŸç±»å‹ï¼ˆtable, text, figure, figure_title, header, footer ç­‰ï¼‰
        - block_bbox: è¾¹ç•Œæ¡† [x1, y1, x2, y2]
        - block_content: å†…å®¹ï¼ˆè¡¨æ ¼ä¸º HTMLï¼Œæ–‡æœ¬ä¸ºçº¯æ–‡æœ¬ï¼‰
        - block_id: åŒºå— ID
        - block_order: åŒºå—é¡ºåº
        
        æ³¨æ„ï¼šæ—§ç‰ˆä½¿ç”¨ label, bbox, contentï¼Œæ–°ç‰ˆä½¿ç”¨ block_label, block_bbox, block_content
        
        ç½®ä¿¡åº¦è¯´æ˜ï¼ˆPaddleOCR 3.x æ”¹è¿›ç‰ˆï¼‰ï¼š
        - è¡¨æ ¼åŒºå—ï¼šä» table_res_list è·å–å¹³å‡ OCR ç½®ä¿¡åº¦
        - éè¡¨æ ¼åŒºå—ï¼šä» overall_ocr_res åŒ¹é…è·å–å¹³å‡ OCR ç½®ä¿¡åº¦
        
        Args:
            block: LayoutBlock å¯¹è±¡ï¼ˆdict-like æˆ–æ™®é€šå¯¹è±¡ï¼‰
            block_confidence: åŒºå—çš„å¹³å‡ OCR ç½®ä¿¡åº¦ï¼ˆè¡¨æ ¼æˆ–æ–‡æœ¬åŒºå—ï¼‰
            
        Returns:
            ç»Ÿä¸€æ ¼å¼çš„å­—å…¸ï¼Œå…¼å®¹æ—§ç‰ˆ PPStructure æ ¼å¼
        """
        try:
            # è·å–åŸºæœ¬å±æ€§ - æ”¯æŒ dict-like å’Œæ™®é€šå¯¹è±¡ä¸¤ç§è®¿é—®æ–¹å¼
            # PPStructureV3 ä½¿ç”¨ block_label, block_bbox, block_content
            label = None
            bbox = None
            content = None
            
            # å°è¯• dict-like è®¿é—®ï¼ˆä¼˜å…ˆï¼‰
            if hasattr(block, '__getitem__'):
                try:
                    label = block.get('block_label') if hasattr(block, 'get') else block['block_label']
                except (KeyError, TypeError):
                    pass
                try:
                    bbox = block.get('block_bbox') if hasattr(block, 'get') else block['block_bbox']
                except (KeyError, TypeError):
                    pass
                try:
                    content = block.get('block_content') if hasattr(block, 'get') else block['block_content']
                except (KeyError, TypeError):
                    pass
            
            # å›é€€åˆ°å±æ€§è®¿é—®ï¼ˆå…¼å®¹æ—§ç‰ˆï¼‰
            if label is None:
                label = getattr(block, 'block_label', None) or getattr(block, 'label', None)
            if bbox is None:
                bbox = getattr(block, 'block_bbox', None) or getattr(block, 'bbox', None)
            if content is None:
                content = getattr(block, 'block_content', None) or getattr(block, 'content', None)
            
            if not label:
                logger.warning(f"Block has no label, skipping. Block type: {type(block)}")
                return None
            
            # æ˜ å°„ label åˆ°æ—§ç‰ˆ type
            type_mapping = {
                'table': 'table',
                'figure': 'figure',
                'figure_title': 'figure_caption',
                'text': 'text',
                'title': 'title',
                'header': 'header',
                'footer': 'footer',
                'reference': 'reference',
                'equation': 'equation',
                'table_title': 'table_caption',
                'chart': 'figure',
                'seal': 'figure',
                'doc_title': 'doc_title',
                'paragraph_title': 'title',  # æ®µè½æ ‡é¢˜æ˜ å°„ä¸ºæ ‡é¢˜
            }
            
            item_type = type_mapping.get(label, label)
            
            # å¤„ç† bbox - å¯èƒ½æ˜¯ numpy array
            if bbox is not None:
                if hasattr(bbox, 'tolist'):
                    bbox = bbox.tolist()
                bbox = list(bbox)
            else:
                bbox = [0, 0, 0, 0]
            
            # æ„å»ºç»“æœå­—å…¸
            # ã€æ–°å¢ã€‘ä¿å­˜åŸå§‹ PPStructureV3 ç±»å‹ (original_struct_type) å’Œç¼–è¾‘ç±»å‹ (edit_type)
            edit_type = 'table' if item_type == 'table' else 'text'
            item_dict = {
                'type': item_type,
                'bbox': bbox,
                'original_struct_type': label,  # PPStructureV3 åŸå§‹ç±»å‹
                'edit_type': edit_type,  # ç¼–è¾‘ç±»å‹: text æˆ– table
            }
            
            # å¤„ç†å†…å®¹
            if item_type == 'table':
                # è¡¨æ ¼å†…å®¹æ˜¯ HTML
                # ä½¿ç”¨ä» table_res_list è·å–çš„å¹³å‡ç½®ä¿¡åº¦
                if content and str(content).strip():
                    item_dict['res'] = {
                        'html': str(content),
                        'confidence': block_confidence  # è¡¨æ ¼å¹³å‡ OCR ç½®ä¿¡åº¦
                    }
                else:
                    item_dict['res'] = {
                        'html': '',
                        'confidence': block_confidence
                    }
                logger.debug(f"Table block: confidence={block_confidence}")
            else:
                # å…¶ä»–ç±»å‹ï¼Œå†…å®¹æ˜¯æ–‡æœ¬
                if content and str(content).strip():
                    content_str = str(content)
                    # ã€ä¿®å¤ã€‘æ£€æŸ¥å†…å®¹æ˜¯å¦æ˜¯ HTML è¡¨æ ¼
                    # å¦‚æœæ˜¯ HTML è¡¨æ ¼ï¼Œåº”è¯¥ä½œä¸º table ç±»å‹å¤„ç†
                    content_lower = content_str.lower().strip()
                    if content_lower.startswith('<html') or content_lower.startswith('<table') or '<table>' in content_lower:
                        # å†…å®¹æ˜¯ HTML è¡¨æ ¼ï¼Œä¿®æ”¹ç±»å‹ä¸º table
                        item_dict['type'] = 'table'
                        item_dict['res'] = {
                            'html': content_str,
                            'confidence': block_confidence  # ä½¿ç”¨åŒºå—ç½®ä¿¡åº¦
                        }
                        logger.debug(f"Non-table block with HTML table content converted to table type")
                    else:
                        # è½¬æ¢ä¸ºæ—§ç‰ˆæ ¼å¼ï¼šres æ˜¯æ–‡æœ¬è¡Œåˆ—è¡¨
                        # ã€æ”¹è¿›ã€‘ä½¿ç”¨ä» overall_ocr_res åŒ¹é…çš„ç½®ä¿¡åº¦
                        item_dict['res'] = [{
                            'text': content_str.strip(),
                            'confidence': block_confidence,  # ä½¿ç”¨ä» overall_ocr_res åŒ¹é…çš„ç½®ä¿¡åº¦
                            'text_region': []
                        }]
                else:
                    item_dict['res'] = []
            
            return item_dict
            
        except Exception as e:
            logger.warning(f"Failed to convert LayoutBlock to dict: {e}")
            import traceback
            logger.warning(traceback.format_exc())
            return None
    
    def _split_large_table(self, table: TableStructure) -> List[TableStructure]:
        """
        Split a large table into smaller tables based on empty rows
        
        Args:
            table: Large table structure to split
            
        Returns:
            List of smaller table structures
        """
        if not table.cells or len(table.cells) <= 5:
            return [table]
        
        tables = []
        current_rows = []
        empty_row_count = 0
        
        for row_idx, row in enumerate(table.cells):
            # Check if row is mostly empty (separator row)
            non_empty_cells = sum(1 for cell in row if cell and cell.strip())
            is_empty_row = non_empty_cells <= 1  # Allow 1 non-empty cell for row numbers
            
            if is_empty_row:
                empty_row_count += 1
                # If we have 2+ consecutive empty rows, it might be a table separator
                if empty_row_count >= 2 and current_rows:
                    # Save current table
                    if len(current_rows) >= 2:  # At least 2 rows to be a table
                        new_table = TableStructure(
                            rows=len(current_rows),
                            columns=table.columns,
                            cells=current_rows,
                            coordinates=table.coordinates,  # Approximate
                            has_headers=True
                        )
                        tables.append(new_table)
                    current_rows = []
                    empty_row_count = 0
            else:
                empty_row_count = 0
                current_rows.append(row)
        
        # Don't forget the last table
        if current_rows and len(current_rows) >= 2:
            new_table = TableStructure(
                rows=len(current_rows),
                columns=table.columns,
                cells=current_rows,
                coordinates=table.coordinates,
                has_headers=True
            )
            tables.append(new_table)
        
        # If no split happened, return original
        if not tables:
            return [table]
        
        logger.info(f"Split table with {table.rows} rows into {len(tables)} tables")
        return tables
    
    def _extract_table_from_region(self, image: np.ndarray, region: Region) -> Optional[TableStructure]:
        """
        Extract table structure from a specific region
        
        Args:
            image: OpenCV image array
            region: Region containing table
            
        Returns:
            TableStructure object or None if extraction fails
        """
        try:
            # Crop table region
            x = int(region.coordinates.x)
            y = int(region.coordinates.y)
            w = int(region.coordinates.width)
            h = int(region.coordinates.height)
            
            # Add padding to ensure complete table capture
            padding = 10
            x = max(0, x - padding)
            y = max(0, y - padding)
            w = min(image.shape[1] - x, w + 2 * padding)
            h = min(image.shape[0] - y, h + 2 * padding)
            
            cropped_table = image[y:y+h, x:x+w]
            
            # Save cropped table temporarily
            temp_path = f"/tmp/table_{hash(str(region.coordinates))}.jpg"
            cv2.imwrite(temp_path, cropped_table)
            
            # Extract table structure
            table_structure = self._analyze_table_structure(temp_path, region.coordinates)
            
            # Clean up temporary file
            try:
                os.remove(temp_path)
            except OSError:
                pass
            
            return table_structure
            
        except Exception as e:
            logger.warning(f"Failed to extract table from region: {e}")
            return None
    
    def _analyze_table_structure(self, table_image_path: str, original_coords: BoundingBox) -> Optional[TableStructure]:
        """
        Analyze table structure using OCR and layout analysis
        
        Args:
            table_image_path: Path to cropped table image
            original_coords: Original coordinates of the table
            
        Returns:
            TableStructure object or None if analysis fails
        """
        try:
            # Perform OCR on table image
            ocr_result = self._ocr_engine.ocr(table_image_path, cls=True)
            
            if not ocr_result or not ocr_result[0]:
                return None
            
            # Parse table cells from OCR result
            cells_data = self._parse_table_cells(ocr_result[0])
            
            if not cells_data:
                return None
            
            # Organize cells into grid structure
            table_grid = self._organize_cells_into_grid(cells_data)
            
            if not table_grid:
                return None
            
            # Detect if table has headers
            has_headers = self._detect_table_headers(table_grid)
            
            return TableStructure(
                rows=len(table_grid),
                columns=len(table_grid[0]) if table_grid else 0,
                cells=table_grid,
                coordinates=original_coords,
                has_headers=has_headers
            )
            
        except Exception as e:
            logger.warning(f"Table structure analysis failed: {e}")
            return None
    
    def _parse_table_cells(self, ocr_result: List) -> List[Dict[str, Any]]:
        """
        Parse OCR result to extract table cell information
        
        Args:
            ocr_result: OCR result from table image
            
        Returns:
            List of cell data dictionaries
        """
        cells = []
        
        for line in ocr_result:
            if len(line) < 2:
                continue
            
            try:
                # Extract cell coordinates
                bbox_coords = line[0]
                x_coords = [point[0] for point in bbox_coords]
                y_coords = [point[1] for point in bbox_coords]
                
                cell_bbox = BoundingBox(
                    x=min(x_coords),
                    y=min(y_coords),
                    width=max(x_coords) - min(x_coords),
                    height=max(y_coords) - min(y_coords)
                )
                
                # Extract cell content
                text_content = line[1][0]
                confidence = line[1][1]
                
                cells.append({
                    'bbox': cell_bbox,
                    'content': text_content.strip(),
                    'confidence': confidence,
                    'center_x': cell_bbox.x + cell_bbox.width / 2,
                    'center_y': cell_bbox.y + cell_bbox.height / 2
                })
                
            except Exception as e:
                logger.warning(f"Failed to parse table cell: {e}")
                continue
        
        return cells
    
    def _organize_cells_into_grid(self, cells_data: List[Dict[str, Any]]) -> List[List[str]]:
        """
        Organize cell data into a 2D grid structure
        
        Args:
            cells_data: List of cell data dictionaries
            
        Returns:
            2D list representing table grid
        """
        if not cells_data:
            return []
        
        try:
            # Sort cells by position (top to bottom, left to right)
            cells_data.sort(key=lambda cell: (cell['center_y'], cell['center_x']))
            
            # Group cells into rows based on Y coordinates
            rows = []
            current_row = []
            current_y = cells_data[0]['center_y']
            y_tolerance = 20  # Pixels tolerance for same row
            
            for cell in cells_data:
                if abs(cell['center_y'] - current_y) <= y_tolerance:
                    current_row.append(cell)
                else:
                    if current_row:
                        # Sort current row by X coordinate
                        current_row.sort(key=lambda c: c['center_x'])
                        rows.append(current_row)
                    current_row = [cell]
                    current_y = cell['center_y']
            
            # Add the last row
            if current_row:
                current_row.sort(key=lambda c: c['center_x'])
                rows.append(current_row)
            
            # Convert to string grid
            max_cols = max(len(row) for row in rows) if rows else 0
            table_grid = []
            
            for row in rows:
                row_data = []
                for i in range(max_cols):
                    if i < len(row):
                        row_data.append(row[i]['content'])
                    else:
                        row_data.append('')  # Empty cell
                table_grid.append(row_data)
            
            return table_grid
            
        except Exception as e:
            logger.warning(f"Failed to organize cells into grid: {e}")
            return []
    
    def _detect_table_headers(self, table_grid: List[List[str]]) -> bool:
        """
        Detect if table has header row based on content analysis
        
        Args:
            table_grid: 2D table grid
            
        Returns:
            True if table likely has headers
        """
        if not table_grid or len(table_grid) < 2:
            return False
        
        try:
            first_row = table_grid[0]
            second_row = table_grid[1] if len(table_grid) > 1 else []
            
            # Heuristics for header detection
            header_indicators = 0
            
            # Check if first row has different formatting patterns
            for i, cell in enumerate(first_row):
                if not cell:
                    continue
                
                # Headers often shorter and more descriptive
                if len(cell) < 50 and any(char.isalpha() for char in cell):
                    header_indicators += 1
                
                # Compare with second row if available
                if i < len(second_row) and second_row[i]:
                    # If first row is text and second row has numbers/data
                    if (cell.replace(' ', '').isalpha() and 
                        any(char.isdigit() for char in second_row[i])):
                        header_indicators += 1
            
            # Consider it a header if more than half the cells show header patterns
            return header_indicators > len(first_row) / 2
            
        except Exception as e:
            logger.warning(f"Header detection failed: {e}")
            return False
    
    def _fallback_table_detection(self, image_path: str) -> List[TableStructure]:
        """
        Fallback table detection using basic OCR and heuristics
        
        Args:
            image_path: Path to image file
            
        Returns:
            List of detected tables using fallback method
        """
        try:
            # Perform regular OCR
            ocr_result = self._ocr_engine.ocr(image_path, cls=True)
            
            if not ocr_result or not ocr_result[0]:
                return []
            
            # Look for table-like patterns in OCR result
            table_candidates = []
            
            for line in ocr_result[0]:
                if len(line) < 2:
                    continue
                
                text_content = line[1][0]
                
                # Simple heuristics for table detection
                if (('\t' in text_content or '|' in text_content or 
                     text_content.count(' ') > len(text_content) * 0.3) and
                    len(text_content.strip()) > 10):
                    
                    # Extract coordinates
                    bbox_coords = line[0]
                    x_coords = [point[0] for point in bbox_coords]
                    y_coords = [point[1] for point in bbox_coords]
                    
                    bbox = BoundingBox(
                        x=min(x_coords),
                        y=min(y_coords),
                        width=max(x_coords) - min(x_coords),
                        height=max(y_coords) - min(y_coords)
                    )
                    
                    # Create simple table structure
                    cells = [cell.strip() for cell in text_content.split() if cell.strip()]
                    if len(cells) >= 2:  # At least 2 columns
                        table_structure = TableStructure(
                            rows=1,
                            columns=len(cells),
                            cells=[cells],
                            coordinates=bbox,
                            has_headers=False
                        )
                        table_candidates.append(table_structure)
            
            return table_candidates
            
        except Exception as e:
            logger.warning(f"Fallback table detection failed: {e}")
            return []
    
    def _parse_table_result(self, table_item: Dict[str, Any]) -> Optional[TableStructure]:
        """
        Parse table result from PP-Structure
        
        Args:
            table_item: Table item from PP-Structure result
            
        Returns:
            TableStructure object or None
        """
        try:
            logger.info(f"Parsing table item with keys: {list(table_item.keys())}")
            
            # Get bounding box if available
            bbox = BoundingBox(0, 0, 0, 0)
            if 'bbox' in table_item:
                box = table_item['bbox']
                if len(box) >= 4:
                    bbox = BoundingBox(
                        x=float(box[0]),
                        y=float(box[1]),
                        width=float(box[2] - box[0]),
                        height=float(box[3] - box[1])
                    )
                    logger.info(f"Table bbox: x={bbox.x}, y={bbox.y}, w={bbox.width}, h={bbox.height}")
            
            # Extract table data from PP-Structure result
            if 'res' not in table_item:
                logger.warning("No 'res' key in table item")
                return None
            
            table_data = table_item['res']
            logger.info(f"Table res type: {type(table_data)}")
            
            table_structure = None
            
            # Parse table structure based on data format
            if isinstance(table_data, dict):
                logger.info(f"Table data dict keys: {list(table_data.keys())}")
                if 'html' in table_data:
                    # Parse HTML table structure (if available)
                    table_structure = self._parse_html_table(table_data['html'])
                elif 'cell_bbox' in table_data:
                    # Parse cell-based table data
                    table_structure = self._parse_cell_bbox_table(table_data)
            elif isinstance(table_data, list):
                # Parse list-based table data
                table_structure = self._parse_list_table(table_data)
            elif isinstance(table_data, str):
                # HTML string directly
                table_structure = self._parse_html_table(table_data)
            
            # Update coordinates if we have them
            if table_structure and bbox.width > 0:
                table_structure.coordinates = bbox
            
            return table_structure
            
        except Exception as e:
            logger.warning(f"Failed to parse table result: {e}")
            import traceback
            logger.warning(traceback.format_exc())
            return None
    
    def _parse_cell_bbox_table(self, table_data: Dict) -> Optional[TableStructure]:
        """
        Parse table from cell bounding box data
        
        Args:
            table_data: Dictionary with cell_bbox and other table info
            
        Returns:
            TableStructure object or None
        """
        try:
            cell_bboxes = table_data.get('cell_bbox', [])
            if not cell_bboxes:
                return None
            
            # Group cells by row (based on y-coordinate)
            rows_dict = {}
            for cell_info in cell_bboxes:
                if len(cell_info) >= 5:
                    # Format: [x1, y1, x2, y2, text] or similar
                    y_center = (cell_info[1] + cell_info[3]) / 2
                    row_key = int(y_center / 20)  # Group by ~20px rows
                    
                    if row_key not in rows_dict:
                        rows_dict[row_key] = []
                    
                    text = str(cell_info[4]) if len(cell_info) > 4 else ''
                    rows_dict[row_key].append((cell_info[0], text))  # (x, text)
            
            # Sort rows and cells
            sorted_rows = sorted(rows_dict.keys())
            table_grid = []
            max_cols = 0
            
            for row_key in sorted_rows:
                cells = sorted(rows_dict[row_key], key=lambda x: x[0])
                row_data = [cell[1] for cell in cells]
                table_grid.append(row_data)
                max_cols = max(max_cols, len(row_data))
            
            # Normalize row lengths
            for row in table_grid:
                while len(row) < max_cols:
                    row.append('')
            
            if not table_grid:
                return None
            
            return TableStructure(
                rows=len(table_grid),
                columns=max_cols,
                cells=table_grid,
                coordinates=BoundingBox(0, 0, 0, 0),
                has_headers=True  # Assume first row is header
            )
            
        except Exception as e:
            logger.warning(f"Cell bbox table parsing failed: {e}")
            return None
    
    def _parse_html_table(self, html_content: str) -> Optional[TableStructure]:
        """
        Parse HTML table content to extract structure
        
        Args:
            html_content: HTML table content
            
        Returns:
            TableStructure object or None
        """
        try:
            from bs4 import BeautifulSoup
            
            soup = BeautifulSoup(html_content, 'html.parser')
            table = soup.find('table')
            
            if not table:
                return None
            
            rows = table.find_all('tr')
            if not rows:
                return None
            
            table_grid = []
            max_cols = 0
            
            for row in rows:
                cells = row.find_all(['td', 'th'])
                row_data = [cell.get_text(strip=True) for cell in cells]
                table_grid.append(row_data)
                max_cols = max(max_cols, len(row_data))
            
            # Normalize row lengths
            for row in table_grid:
                while len(row) < max_cols:
                    row.append('')
            
            # Detect headers (first row with th tags)
            has_headers = bool(rows[0].find_all('th')) if rows else False
            
            return TableStructure(
                rows=len(table_grid),
                columns=max_cols,
                cells=table_grid,
                coordinates=BoundingBox(0, 0, 0, 0),  # Will be updated with actual coordinates
                has_headers=has_headers
            )
            
        except ImportError:
            logger.warning("BeautifulSoup not available for HTML table parsing")
            return None
        except Exception as e:
            logger.warning(f"HTML table parsing failed: {e}")
            return None
    
    def _parse_list_table(self, table_data: List) -> Optional[TableStructure]:
        """
        Parse list-based table data
        
        Args:
            table_data: List-based table data
            
        Returns:
            TableStructure object or None
        """
        try:
            if not table_data:
                return None
            
            # Convert list data to grid format
            table_grid = []
            max_cols = 0
            
            for row_data in table_data:
                if isinstance(row_data, list):
                    row = [str(cell) for cell in row_data]
                    table_grid.append(row)
                    max_cols = max(max_cols, len(row))
                elif isinstance(row_data, str):
                    # Split string into cells
                    row = [cell.strip() for cell in row_data.split('\t') if cell.strip()]
                    if not row:
                        row = [cell.strip() for cell in row_data.split() if cell.strip()]
                    table_grid.append(row)
                    max_cols = max(max_cols, len(row))
            
            # Normalize row lengths
            for row in table_grid:
                while len(row) < max_cols:
                    row.append('')
            
            if not table_grid:
                return None
            
            return TableStructure(
                rows=len(table_grid),
                columns=max_cols,
                cells=table_grid,
                coordinates=BoundingBox(0, 0, 0, 0),  # Will be updated with actual coordinates
                has_headers=self._detect_table_headers(table_grid)
            )
            
        except Exception as e:
            logger.warning(f"List table parsing failed: {e}")
            return None