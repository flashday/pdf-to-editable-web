"""
ChatOCR Service - PP-ChatOCRv4 æ™ºèƒ½æ–‡æ¡£ç†è§£æœåŠ¡

æ•´åˆ LLM æœåŠ¡å’Œ RAG æœåŠ¡ï¼Œæä¾›å…³é”®ä¿¡æ¯æå–å’Œæ–‡æ¡£é—®ç­”åŠŸèƒ½
"""
import json
import logging
import time
import re
import os
from pathlib import Path
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, field
from datetime import datetime

from backend.services.llm_service import get_llm_service, OllamaLLMService
from backend.services.rag_service import get_rag_service, RAGService
from backend.services.job_cache import get_job_cache
from backend.config import ChatOCRConfig

logger = logging.getLogger(__name__)


def get_temp_folder() -> Path:
    """è·å–ä¸´æ—¶æ–‡ä»¶å¤¹è·¯å¾„"""
    return Path(os.environ.get('TEMP_FOLDER', 'temp'))


# ============== é¢„è®¾æ¨¡æ¿å®šä¹‰ ==============

EXTRACTION_TEMPLATES = {
    "invoice": {
        "name": "å‘ç¥¨",
        "name_en": "Invoice",
        "fields": ["å‘ç¥¨å·ç ", "å‘ç¥¨ä»£ç ", "å¼€ç¥¨æ—¥æœŸ", "é‡‘é¢åˆè®¡", "ç¨é¢", "ä»·ç¨åˆè®¡", "è´­ä¹°æ–¹åç§°", "é”€å”®æ–¹åç§°"],
        "prompt_hint": "è¿™æ˜¯ä¸€ä»½å‘ç¥¨æ–‡æ¡£ï¼Œè¯·ä»”ç»†è¯†åˆ«å‘ç¥¨ä¸Šçš„å„é¡¹ä¿¡æ¯"
    },
    "contract": {
        "name": "åˆåŒ",
        "name_en": "Contract",
        "fields": ["ç”²æ–¹", "ä¹™æ–¹", "åˆåŒé‡‘é¢", "ç­¾è®¢æ—¥æœŸ", "åˆåŒæœŸé™", "åˆåŒç¼–å·", "è¿çº¦æ¡æ¬¾"],
        "prompt_hint": "è¿™æ˜¯ä¸€ä»½åˆåŒæ–‡æ¡£ï¼Œè¯·æå–åˆåŒä¸­çš„å…³é”®æ¡æ¬¾ä¿¡æ¯"
    },
    "id_card": {
        "name": "èº«ä»½è¯",
        "name_en": "ID Card",
        "fields": ["å§“å", "æ€§åˆ«", "æ°‘æ—", "å‡ºç”Ÿæ—¥æœŸ", "ä½å€", "èº«ä»½è¯å·ç "],
        "prompt_hint": "è¿™æ˜¯ä¸€ä»½èº«ä»½è¯æ–‡æ¡£ï¼Œè¯·æå–èº«ä»½è¯ä¸Šçš„ä¸ªäººä¿¡æ¯"
    },
    "resume": {
        "name": "ç®€å†",
        "name_en": "Resume",
        "fields": ["å§“å", "è”ç³»ç”µè¯", "ç”µå­é‚®ç®±", "æ•™è‚²èƒŒæ™¯", "å·¥ä½œç»å†", "æŠ€èƒ½ç‰¹é•¿"],
        "prompt_hint": "è¿™æ˜¯ä¸€ä»½ä¸ªäººç®€å†ï¼Œè¯·æå–ç®€å†ä¸­çš„å…³é”®ä¿¡æ¯"
    },
    "receipt": {
        "name": "æ”¶æ®",
        "name_en": "Receipt",
        "fields": ["æ”¶æ®ç¼–å·", "æ—¥æœŸ", "ä»˜æ¬¾äºº", "æ”¶æ¬¾äºº", "é‡‘é¢", "äº‹ç”±"],
        "prompt_hint": "è¿™æ˜¯ä¸€ä»½æ”¶æ®æ–‡æ¡£ï¼Œè¯·æå–æ”¶æ®ä¸Šçš„äº¤æ˜“ä¿¡æ¯"
    },
    "business_license": {
        "name": "è¥ä¸šæ‰§ç…§",
        "name_en": "Business License",
        "fields": ["ä¼ä¸šåç§°", "ç»Ÿä¸€ç¤¾ä¼šä¿¡ç”¨ä»£ç ", "æ³•å®šä»£è¡¨äºº", "æ³¨å†Œèµ„æœ¬", "æˆç«‹æ—¥æœŸ", "è¥ä¸šæœŸé™", "ç»è¥èŒƒå›´"],
        "prompt_hint": "è¿™æ˜¯ä¸€ä»½è¥ä¸šæ‰§ç…§ï¼Œè¯·æå–ä¼ä¸šçš„æ³¨å†Œä¿¡æ¯"
    }
}


# ============== Prompt æ¨¡æ¿ ==============

EXTRACTION_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£ä¿¡æ¯æå–åŠ©æ‰‹ã€‚è¯·ä»ä»¥ä¸‹æ–‡æ¡£å†…å®¹ä¸­æå–æŒ‡å®šçš„å­—æ®µä¿¡æ¯ã€‚

{prompt_hint}

æ–‡æ¡£å†…å®¹ï¼š
{document_content}

éœ€è¦æå–çš„å­—æ®µï¼š
{fields}

è¯·æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¿”å›ç»“æœï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–è¯´æ˜æ–‡å­—ï¼š
{{
{field_template}
}}

æ³¨æ„äº‹é¡¹ï¼š
1. å¦‚æœæŸä¸ªå­—æ®µåœ¨æ–‡æ¡£ä¸­æ‰¾ä¸åˆ°ï¼Œè¿”å› null
2. ä¿æŒæå–å€¼çš„åŸå§‹æ ¼å¼ï¼ˆå¦‚æ—¥æœŸã€é‡‘é¢æ ¼å¼ï¼‰
3. åªè¿”å› JSONï¼Œä¸è¦æ·»åŠ  markdown ä»£ç å—æ ‡è®°æˆ–å…¶ä»–è¯´æ˜
4. ç¡®ä¿ JSON æ ¼å¼æ­£ç¡®ï¼Œå¯ä»¥è¢«è§£æ"""

QA_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£é—®ç­”åŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

æ–‡æ¡£å†…å®¹ï¼š
{document_content}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·æŒ‰ç…§ä»¥ä¸‹è¦æ±‚å›ç­”ï¼š
1. åªåŸºäºæ–‡æ¡£å†…å®¹å›ç­”ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯
2. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®å‘ŠçŸ¥
3. åœ¨å›ç­”ä¸­å¼•ç”¨æ–‡æ¡£åŸæ–‡ä½œä¸ºä¾æ®
4. ä½¿ç”¨ç®€æ´æ¸…æ™°çš„è¯­è¨€

è¯·æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¿”å›ï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–è¯´æ˜æ–‡å­—ï¼š
{{
    "answer": "ä½ çš„å›ç­”",
    "references": ["å¼•ç”¨çš„åŸæ–‡ç‰‡æ®µ1", "å¼•ç”¨çš„åŸæ–‡ç‰‡æ®µ2"],
    "found_in_document": trueæˆ–false
}}

æ³¨æ„ï¼šåªè¿”å› JSONï¼Œä¸è¦æ·»åŠ  markdown ä»£ç å—æ ‡è®°æˆ–å…¶ä»–è¯´æ˜"""


# ============== æ•°æ®æ¨¡å‹ ==============

@dataclass
class ExtractionResult:
    """æå–ç»“æœ"""
    job_id: str
    fields: Dict[str, Optional[str]]
    confidence: float = 0.0
    warnings: List[str] = field(default_factory=list)
    processing_time: float = 0.0
    timestamp: datetime = field(default_factory=datetime.now)
    success: bool = True
    error: Optional[str] = None
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "job_id": self.job_id,
            "fields": self.fields,
            "confidence": self.confidence,
            "warnings": self.warnings,
            "processing_time": self.processing_time,
            "timestamp": self.timestamp.isoformat(),
            "success": self.success,
            "error": self.error
        }


@dataclass
class QAResult:
    """é—®ç­”ç»“æœ"""
    job_id: str
    question: str
    answer: str
    references: List[str] = field(default_factory=list)
    confidence: float = 0.0
    processing_time: float = 0.0
    timestamp: datetime = field(default_factory=datetime.now)
    success: bool = True
    error: Optional[str] = None
    found_in_document: bool = True
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "job_id": self.job_id,
            "question": self.question,
            "answer": self.answer,
            "references": self.references,
            "confidence": self.confidence,
            "processing_time": self.processing_time,
            "timestamp": self.timestamp.isoformat(),
            "success": self.success,
            "error": self.error,
            "found_in_document": self.found_in_document
        }


# ============== ChatOCR æœåŠ¡ ==============

class ChatOCRService:
    """PP-ChatOCRv4 æ™ºèƒ½æ–‡æ¡£ç†è§£æœåŠ¡"""
    
    def __init__(
        self,
        llm_service: Optional[OllamaLLMService] = None,
        rag_service: Optional[RAGService] = None
    ):
        """
        åˆå§‹åŒ– ChatOCR æœåŠ¡
        
        Args:
            llm_service: LLM æœåŠ¡å®ä¾‹ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨å…¨å±€å®ä¾‹
            rag_service: RAG æœåŠ¡å®ä¾‹ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨å…¨å±€å®ä¾‹
        """
        self._llm_service = llm_service
        self._rag_service = rag_service
        self.templates = EXTRACTION_TEMPLATES
        
        logger.info("ChatOCR service initialized")
    
    @property
    def llm_service(self) -> Optional[OllamaLLMService]:
        """è·å– LLM æœåŠ¡"""
        if self._llm_service is None:
            self._llm_service = get_llm_service()
        return self._llm_service
    
    @property
    def rag_service(self) -> Optional[RAGService]:
        """è·å– RAG æœåŠ¡"""
        if self._rag_service is None and ChatOCRConfig.ENABLE_RAG:
            self._rag_service = get_rag_service()
        return self._rag_service
    
    def check_available(self) -> Dict[str, Any]:
        """
        æ£€æŸ¥æœåŠ¡æ˜¯å¦å¯ç”¨
        
        Returns:
            dict: åŒ…å« available, llm_status, rag_status çš„çŠ¶æ€ä¿¡æ¯
        """
        llm_available = False
        rag_available = False
        
        # æ£€æŸ¥ LLM æœåŠ¡
        if self.llm_service:
            llm_available = self.llm_service.check_health()
        
        # æ£€æŸ¥ RAG æœåŠ¡
        if self.rag_service and ChatOCRConfig.ENABLE_RAG:
            rag_available = True  # RAG æœåŠ¡æ˜¯æœ¬åœ°çš„ï¼Œé€šå¸¸å¯ç”¨
        
        return {
            "available": llm_available,
            "llm_available": llm_available,
            "rag_available": rag_available,
            "model": ChatOCRConfig.OLLAMA_MODEL,
            "base_url": ChatOCRConfig.OLLAMA_BASE_URL,
            "chatocr_enabled": ChatOCRConfig.ENABLE_CHATOCR,
            "rag_enabled": ChatOCRConfig.ENABLE_RAG
        }
    
    def get_templates(self) -> Dict[str, Dict[str, Any]]:
        """
        è·å–é¢„è®¾æå–æ¨¡æ¿
        
        Returns:
            dict: æ¨¡æ¿å­—å…¸ï¼Œkey ä¸ºæ¨¡æ¿ IDï¼Œvalue ä¸ºæ¨¡æ¿è¯¦æƒ…
        """
        return {
            template_id: {
                "name": template["name"],
                "name_en": template["name_en"],
                "fields": template["fields"]
            }
            for template_id, template in self.templates.items()
        }
    
    def get_template_fields(self, template_id: str) -> Optional[List[str]]:
        """
        è·å–æŒ‡å®šæ¨¡æ¿çš„å­—æ®µåˆ—è¡¨
        
        Args:
            template_id: æ¨¡æ¿ ID
            
        Returns:
            list: å­—æ®µåˆ—è¡¨ï¼Œå¦‚æœæ¨¡æ¿ä¸å­˜åœ¨è¿”å› None
        """
        template = self.templates.get(template_id)
        return template["fields"] if template else None
    
    def _save_extraction_log(self, job_id: str, result: 'ExtractionResult', 
                             fields: List[str], template: Optional[str] = None) -> None:
        """
        ä¿å­˜æ™ºèƒ½æå–æ—¥å¿—åˆ° temp ç›®å½•ï¼ˆJSON æ ¼å¼ï¼‰
        
        Args:
            job_id: ä»»åŠ¡ ID
            result: æå–ç»“æœ
            fields: æå–çš„å­—æ®µåˆ—è¡¨
            template: ä½¿ç”¨çš„æ¨¡æ¿åç§°
        """
        try:
            temp_folder = get_temp_folder()
            log_path = temp_folder / f"{job_id}_extract_log.json"
            
            # è¯»å–ç°æœ‰æ—¥å¿—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            existing_logs = []
            if log_path.exists():
                with open(log_path, 'r', encoding='utf-8') as f:
                    existing_logs = json.load(f)
            
            # æ·»åŠ æ–°çš„æå–è®°å½•
            log_entry = {
                "timestamp": datetime.now().isoformat(),
                "template": template,
                "fields_requested": fields,
                "result": result.to_dict(),
                "model": ChatOCRConfig.OLLAMA_MODEL
            }
            existing_logs.append(log_entry)
            
            # ä¿å­˜æ—¥å¿—
            with open(log_path, 'w', encoding='utf-8') as f:
                json.dump(existing_logs, f, ensure_ascii=False, indent=2)
            
            logger.info(f"Extraction log saved to {log_path}")
            
        except Exception as e:
            logger.warning(f"Failed to save extraction log: {e}")
    
    def _save_qa_log(self, job_id: str, result: 'QAResult') -> None:
        """
        ä¿å­˜é—®ç­”æ—¥å¿—åˆ° temp ç›®å½•ï¼ˆMarkdown æ ¼å¼ï¼‰
        
        Args:
            job_id: ä»»åŠ¡ ID
            result: é—®ç­”ç»“æœ
        """
        try:
            temp_folder = get_temp_folder()
            log_path = temp_folder / f"{job_id}_qa_log.md"
            
            # æ„å»º Markdown å†…å®¹
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            
            # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ·»åŠ å¤´éƒ¨
            if not log_path.exists():
                header = f"""# æ–‡æ¡£é—®ç­”æ—¥å¿—

**Job ID**: {job_id}
**LLM æ¨¡å‹**: {ChatOCRConfig.OLLAMA_MODEL}
**Embedding æ¨¡å‹**: {ChatOCRConfig.EMBEDDING_MODEL}

---

"""
                with open(log_path, 'w', encoding='utf-8') as f:
                    f.write(header)
            
            # è¿½åŠ é—®ç­”è®°å½•
            qa_entry = f"""
## é—®ç­”è®°å½• [{timestamp}]

### ğŸ™‹ ç”¨æˆ·é—®é¢˜

{result.question}

### ğŸ¤– AI å›ç­”

{result.answer}

"""
            # æ·»åŠ å‚è€ƒåŸæ–‡
            if result.references:
                qa_entry += "### ğŸ“ å‚è€ƒåŸæ–‡\n\n"
                for i, ref in enumerate(result.references, 1):
                    qa_entry += f"> {i}. \"{ref}\"\n\n"
            
            # æ·»åŠ å…ƒæ•°æ®
            qa_entry += f"""### ğŸ“Š å…ƒæ•°æ®

| æŒ‡æ ‡ | å€¼ |
|------|-----|
| ç½®ä¿¡åº¦ | {result.confidence * 100:.1f}% |
| å¤„ç†æ—¶é—´ | {result.processing_time:.2f}s |
| æ–‡æ¡£ä¸­æ‰¾åˆ° | {'æ˜¯' if result.found_in_document else 'å¦'} |
| æˆåŠŸ | {'æ˜¯' if result.success else 'å¦'} |

---

"""
            
            with open(log_path, 'a', encoding='utf-8') as f:
                f.write(qa_entry)
            
            logger.info(f"QA log saved to {log_path}")
            
        except Exception as e:
            logger.warning(f"Failed to save QA log: {e}")

    
    def _get_document_content(self, job_id: str, query: Optional[str] = None) -> Optional[str]:
        """
        è·å–æ–‡æ¡£å†…å®¹
        
        å¦‚æœå¯ç”¨äº† RAG ä¸”æœ‰æŸ¥è¯¢ï¼Œåˆ™ä½¿ç”¨å‘é‡æ£€ç´¢è·å–ç›¸å…³ç‰‡æ®µ
        å¦åˆ™ä»ç¼“å­˜è·å–å®Œæ•´æ–‡æ¡£å†…å®¹
        
        Args:
            job_id: æ–‡æ¡£ job_id
            query: æŸ¥è¯¢æ–‡æœ¬ï¼ˆç”¨äº RAG æ£€ç´¢ï¼‰
            
        Returns:
            str: æ–‡æ¡£å†…å®¹ï¼Œå¦‚æœè·å–å¤±è´¥è¿”å› None
        """
        # å°è¯•ä½¿ç”¨ RAG æ£€ç´¢ç›¸å…³å†…å®¹
        if query and self.rag_service and ChatOCRConfig.ENABLE_RAG:
            try:
                results = self.rag_service.retrieve(job_id, query, top_k=5)
                if results and results.chunks:
                    # ä» chunks ä¸­æå–æ–‡æ¡£å†…å®¹
                    documents = [chunk.get("document", "") for chunk in results.chunks if chunk.get("document")]
                    if documents:
                        content = "\n\n".join(documents)
                        logger.info(f"Retrieved {len(documents)} chunks via RAG for job {job_id}")
                        return content
            except Exception as e:
                logger.warning(f"RAG retrieval failed, falling back to full content: {e}")
        
        # ä»ç¼“å­˜è·å–å®Œæ•´å†…å®¹
        try:
            import os
            from pathlib import Path
            temp_folder = Path(os.environ.get('TEMP_FOLDER', 'temp'))
            
            # æ–¹æ³•1: å°è¯•ä» ppstructure.json è·å–æ–‡æœ¬
            ppstructure_path = temp_folder / f"{job_id}_ppstructure.json"
            if ppstructure_path.exists():
                with open(ppstructure_path, 'r', encoding='utf-8') as f:
                    ppstructure_data = json.load(f)
                    items = ppstructure_data.get('items', [])
                    texts = []
                    for item in items:
                        res = item.get('res', {})
                        text = self._extract_text_from_res(res)
                        if text:
                            texts.append(text)
                    if texts:
                        content = '\n\n'.join(texts)
                        logger.info(f"Extracted {len(texts)} text blocks from ppstructure.json for job {job_id}")
                        return content
            
            # æ–¹æ³•2: å°è¯•è¯»å– raw_ocr.json
            raw_ocr_path = temp_folder / f"{job_id}_raw_ocr.json"
            if raw_ocr_path.exists():
                with open(raw_ocr_path, 'r', encoding='utf-8') as f:
                    ocr_data = json.load(f)
                    if isinstance(ocr_data, dict) and 'text' in ocr_data:
                        return ocr_data['text']
                    elif isinstance(ocr_data, list):
                        # ä» OCR ç»“æœåˆ—è¡¨æå–æ–‡æœ¬
                        texts = []
                        for item in ocr_data:
                            if isinstance(item, dict) and 'text' in item:
                                texts.append(item['text'])
                            elif isinstance(item, str):
                                texts.append(item)
                        if texts:
                            return '\n'.join(texts)
            
            # æ–¹æ³•3: å°è¯•è¯»å– raw_ocr.html å¹¶æå–æ–‡æœ¬
            raw_html_path = temp_folder / f"{job_id}_raw_ocr.html"
            if raw_html_path.exists():
                with open(raw_html_path, 'r', encoding='utf-8') as f:
                    html_content = f.read()
                    return self._extract_text_from_html(html_content)
                    
        except Exception as e:
            logger.error(f"Failed to get document content for job {job_id}: {e}")
        
        return None
    
    def _extract_text_from_res(self, res: Any) -> str:
        """ä» ppstructure res å­—æ®µæå–æ–‡æœ¬"""
        if isinstance(res, str):
            return res
        if isinstance(res, list):
            texts = []
            for item in res:
                if isinstance(item, dict) and 'text' in item:
                    texts.append(item['text'])
                elif isinstance(item, str):
                    texts.append(item)
            return ' '.join(texts)
        if isinstance(res, dict):
            if 'text' in res:
                return res['text']
            # å¯¹äºè¡¨æ ¼ï¼Œå°è¯•ä» html æå–æ–‡æœ¬
            if 'html' in res:
                return self._extract_text_from_html(res['html'])
        return ''
    
    def _extract_text_from_html(self, html: str) -> str:
        """ä» HTML ä¸­æå–çº¯æ–‡æœ¬"""
        try:
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html, 'html.parser')
            return soup.get_text(separator='\n', strip=True)
        except Exception:
            # ç®€å•çš„æ­£åˆ™æå–
            text = re.sub(r'<[^>]+>', ' ', html)
            return ' '.join(text.split())
    
    def _parse_json_response(self, response: str) -> Optional[Dict]:
        """
        è§£æ LLM è¿”å›çš„ JSON å“åº”
        
        Args:
            response: LLM å“åº”æ–‡æœ¬
            
        Returns:
            dict: è§£æåçš„ JSON å¯¹è±¡ï¼Œè§£æå¤±è´¥è¿”å› None
        """
        if not response:
            return None
        
        # æ¸…ç†å“åº”æ–‡æœ¬
        text = response.strip()
        
        # ç§»é™¤å¯èƒ½çš„ markdown ä»£ç å—æ ‡è®°
        if text.startswith('```json'):
            text = text[7:]
        elif text.startswith('```'):
            text = text[3:]
        if text.endswith('```'):
            text = text[:-3]
        
        text = text.strip()
        
        # å°è¯•ç›´æ¥è§£æ
        try:
            return json.loads(text)
        except json.JSONDecodeError:
            pass
        
        # å°è¯•æ‰¾åˆ° JSON å¯¹è±¡
        json_match = re.search(r'\{[\s\S]*\}', text)
        if json_match:
            try:
                return json.loads(json_match.group())
            except json.JSONDecodeError:
                pass
        
        logger.warning(f"Failed to parse JSON response: {text[:200]}...")
        return None
    
    def extract_info(
        self,
        job_id: str,
        fields: Optional[List[str]] = None,
        template: Optional[str] = None
    ) -> ExtractionResult:
        """
        ä»æ–‡æ¡£ä¸­æå–å…³é”®ä¿¡æ¯
        
        Args:
            job_id: æ–‡æ¡£ job_id
            fields: è¦æå–çš„å­—æ®µåˆ—è¡¨
            template: é¢„è®¾æ¨¡æ¿åç§°
            
        Returns:
            ExtractionResult: æå–ç»“æœ
        """
        start_time = time.time()
        
        # ç¡®å®šè¦æå–çš„å­—æ®µ
        if template and template in self.templates:
            template_info = self.templates[template]
            extract_fields = fields or template_info["fields"]
            prompt_hint = template_info["prompt_hint"]
        else:
            extract_fields = fields or []
            prompt_hint = "è¯·ä»”ç»†åˆ†ææ–‡æ¡£å†…å®¹"
        
        if not extract_fields:
            return ExtractionResult(
                job_id=job_id,
                fields={},
                success=False,
                error="æœªæŒ‡å®šæå–å­—æ®µ",
                processing_time=time.time() - start_time
            )
        
        # æ£€æŸ¥ LLM æœåŠ¡
        if not self.llm_service or not self.llm_service.check_health():
            return ExtractionResult(
                job_id=job_id,
                fields={field: None for field in extract_fields},
                success=False,
                error="LLM æœåŠ¡ä¸å¯ç”¨",
                warnings=["æ™ºèƒ½æå–åŠŸèƒ½æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•"],
                processing_time=time.time() - start_time
            )
        
        # è·å–æ–‡æ¡£å†…å®¹
        query_hint = "ã€".join(extract_fields[:3])  # ç”¨å‰å‡ ä¸ªå­—æ®µä½œä¸ºæ£€ç´¢æç¤º
        document_content = self._get_document_content(job_id, query_hint)
        
        if not document_content:
            return ExtractionResult(
                job_id=job_id,
                fields={field: None for field in extract_fields},
                success=False,
                error="æ— æ³•è·å–æ–‡æ¡£å†…å®¹",
                processing_time=time.time() - start_time
            )
        
        # æ„å»º prompt
        field_template = ",\n".join([f'    "{field}": "æå–çš„å€¼æˆ–null"' for field in extract_fields])
        prompt = EXTRACTION_PROMPT.format(
            prompt_hint=prompt_hint,
            document_content=document_content[:ChatOCRConfig.LLM_CONTEXT_LIMIT],  # ä½¿ç”¨é…ç½®çš„ä¸Šä¸‹æ–‡é™åˆ¶
            fields=", ".join(extract_fields),
            field_template=field_template
        )
        
        # è°ƒç”¨ LLM
        try:
            messages = [{"role": "user", "content": prompt}]
            llm_response = self.llm_service.chat(messages)
            
            # æ£€æŸ¥ LLM å“åº”æ˜¯å¦æˆåŠŸ
            if not llm_response.success:
                return ExtractionResult(
                    job_id=job_id,
                    fields={field: None for field in extract_fields},
                    success=False,
                    error=llm_response.error_message or "LLM è¯·æ±‚å¤±è´¥",
                    processing_time=time.time() - start_time
                )
            
            # ä» LLMResponse å¯¹è±¡ä¸­æå–æ–‡æœ¬å†…å®¹
            response_text = llm_response.content
            
            # è§£æå“åº”
            result = self._parse_json_response(response_text)
            
            if result:
                # ç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½æœ‰å€¼
                extracted_fields = {}
                warnings = []
                null_count = 0
                
                for field in extract_fields:
                    value = result.get(field)
                    extracted_fields[field] = value
                    if value is None:
                        null_count += 1
                
                # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºæˆåŠŸæå–çš„å­—æ®µæ¯”ä¾‹ï¼‰
                confidence = (len(extract_fields) - null_count) / len(extract_fields) if extract_fields else 0
                
                if null_count > 0:
                    warnings.append(f"æœ‰ {null_count} ä¸ªå­—æ®µæœªèƒ½ä»æ–‡æ¡£ä¸­æ‰¾åˆ°")
                
                extraction_result = ExtractionResult(
                    job_id=job_id,
                    fields=extracted_fields,
                    confidence=confidence,
                    warnings=warnings,
                    processing_time=time.time() - start_time,
                    success=True
                )
                
                # ä¿å­˜æå–æ—¥å¿—
                self._save_extraction_log(job_id, extraction_result, extract_fields, template)
                
                return extraction_result
            else:
                extraction_result = ExtractionResult(
                    job_id=job_id,
                    fields={field: None for field in extract_fields},
                    success=False,
                    error="æ— æ³•è§£æ LLM å“åº”",
                    warnings=["LLM è¿”å›çš„ç»“æœæ ¼å¼ä¸æ­£ç¡®"],
                    processing_time=time.time() - start_time
                )
                
                # ä¿å­˜æå–æ—¥å¿—ï¼ˆå³ä½¿å¤±è´¥ä¹Ÿä¿å­˜ï¼‰
                self._save_extraction_log(job_id, extraction_result, extract_fields, template)
                
                return extraction_result
                
        except Exception as e:
            logger.error(f"Extraction failed for job {job_id}: {e}")
            return ExtractionResult(
                job_id=job_id,
                fields={field: None for field in extract_fields},
                success=False,
                error=str(e),
                processing_time=time.time() - start_time
            )

    
    def document_qa(self, job_id: str, question: str) -> QAResult:
        """
        åŸºäºæ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜
        
        Args:
            job_id: æ–‡æ¡£ job_id
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            QAResult: é—®ç­”ç»“æœ
        """
        start_time = time.time()
        
        if not question or not question.strip():
            return QAResult(
                job_id=job_id,
                question=question,
                answer="",
                success=False,
                error="é—®é¢˜ä¸èƒ½ä¸ºç©º",
                processing_time=time.time() - start_time
            )
        
        # æ£€æŸ¥ LLM æœåŠ¡
        if not self.llm_service or not self.llm_service.check_health():
            return QAResult(
                job_id=job_id,
                question=question,
                answer="æ™ºèƒ½é—®ç­”åŠŸèƒ½æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•ã€‚",
                success=False,
                error="LLM æœåŠ¡ä¸å¯ç”¨",
                processing_time=time.time() - start_time
            )
        
        # è·å–æ–‡æ¡£å†…å®¹ï¼ˆä½¿ç”¨é—®é¢˜ä½œä¸ºæ£€ç´¢æŸ¥è¯¢ï¼‰
        document_content = self._get_document_content(job_id, question)
        
        if not document_content:
            return QAResult(
                job_id=job_id,
                question=question,
                answer="æ— æ³•è·å–æ–‡æ¡£å†…å®¹ï¼Œè¯·ç¡®ä¿æ–‡æ¡£å·²å®Œæˆ OCR å¤„ç†ã€‚",
                success=False,
                error="æ— æ³•è·å–æ–‡æ¡£å†…å®¹",
                processing_time=time.time() - start_time
            )
        
        # æ„å»º prompt
        prompt = QA_PROMPT.format(
            document_content=document_content[:ChatOCRConfig.LLM_CONTEXT_LIMIT],  # ä½¿ç”¨é…ç½®çš„ä¸Šä¸‹æ–‡é™åˆ¶
            question=question
        )
        
        # è°ƒç”¨ LLM
        try:
            messages = [{"role": "user", "content": prompt}]
            llm_response = self.llm_service.chat(messages)
            
            # æ£€æŸ¥ LLM å“åº”æ˜¯å¦æˆåŠŸ
            if not llm_response.success:
                return QAResult(
                    job_id=job_id,
                    question=question,
                    answer="LLM æœåŠ¡è¯·æ±‚å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•ã€‚",
                    success=False,
                    error=llm_response.error_message or "LLM è¯·æ±‚å¤±è´¥",
                    processing_time=time.time() - start_time
                )
            
            # ä» LLMResponse å¯¹è±¡ä¸­æå–æ–‡æœ¬å†…å®¹
            response_text = llm_response.content
            
            # è§£æå“åº”
            result = self._parse_json_response(response_text)
            
            if result:
                answer = result.get("answer", "")
                references = result.get("references", [])
                found_in_document = result.get("found_in_document", True)
                
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯
                if not found_in_document or not answer:
                    answer = answer or "æ— æ³•ä»æ–‡æ¡£ä¸­æ‰¾åˆ°ä¸æ‚¨é—®é¢˜ç›¸å…³çš„ä¿¡æ¯ã€‚"
                
                # è®¡ç®—ç½®ä¿¡åº¦
                confidence = 0.9 if found_in_document and references else 0.5
                
                qa_result = QAResult(
                    job_id=job_id,
                    question=question,
                    answer=answer,
                    references=references if isinstance(references, list) else [],
                    confidence=confidence,
                    found_in_document=found_in_document,
                    processing_time=time.time() - start_time,
                    success=True
                )
                
                # ä¿å­˜é—®ç­”æ—¥å¿—
                self._save_qa_log(job_id, qa_result)
                
                return qa_result
            else:
                # å¦‚æœæ— æ³•è§£æ JSONï¼Œå°è¯•ç›´æ¥ä½¿ç”¨å“åº”ä½œä¸ºç­”æ¡ˆ
                qa_result = QAResult(
                    job_id=job_id,
                    question=question,
                    answer=response_text[:1000] if response_text else "æ— æ³•ç”Ÿæˆå›ç­”",
                    confidence=0.3,
                    processing_time=time.time() - start_time,
                    success=True
                )
                
                # ä¿å­˜é—®ç­”æ—¥å¿—
                self._save_qa_log(job_id, qa_result)
                
                return qa_result
                
        except Exception as e:
            logger.error(f"Document QA failed for job {job_id}: {e}")
            return QAResult(
                job_id=job_id,
                question=question,
                answer="å¤„ç†é—®é¢˜æ—¶å‘ç”Ÿé”™è¯¯ï¼Œè¯·ç¨åé‡è¯•ã€‚",
                success=False,
                error=str(e),
                processing_time=time.time() - start_time
            )


# ============== å…¨å±€å®ä¾‹ç®¡ç† ==============

_chatocr_service: Optional[ChatOCRService] = None


def get_chatocr_service() -> Optional[ChatOCRService]:
    """è·å–å…¨å±€ ChatOCR æœåŠ¡å®ä¾‹"""
    global _chatocr_service
    
    if not ChatOCRConfig.ENABLE_CHATOCR:
        return None
    
    if _chatocr_service is None:
        _chatocr_service = ChatOCRService()
    
    return _chatocr_service


def init_chatocr_service(
    llm_service: Optional[OllamaLLMService] = None,
    rag_service: Optional[RAGService] = None
) -> ChatOCRService:
    """
    åˆå§‹åŒ–å…¨å±€ ChatOCR æœåŠ¡
    
    Args:
        llm_service: LLM æœåŠ¡å®ä¾‹
        rag_service: RAG æœåŠ¡å®ä¾‹
        
    Returns:
        ChatOCRService: åˆå§‹åŒ–çš„æœåŠ¡å®ä¾‹
    """
    global _chatocr_service
    _chatocr_service = ChatOCRService(llm_service, rag_service)
    return _chatocr_service
